{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"/Users/minhajul/Downloads/2208.03274.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PyPDF2.PdfReader(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for pi, p in enumerate(reader.pages):\n",
    "    texts.append(p.extract_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76550"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join text into string\n",
    "full_text = \" \".join(texts)\n",
    "len(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Holistic Approach to Undesired Content Detection in the Real World\\nWarning: some content may contain racism, sexuality, or other harmful language.\\nTodor Markov*Chong Zhang*Sandhini Agarwal Tyna Eloundou\\nTeddy Lee Steven Adler Angela Jiang Lilian Weng*\\nOpenAI\\nAbstract\\nWe present a holistic approach to building a robust and useful\\nnatural language classiﬁcation system for real-world content\\nmoderation. The success of such a system relies on a chain of\\ncarefully designed and executed steps, including the design\\nof content taxonomies and labeling instructions, data qual-\\nity control, an active learning pipeline to capture rare events,\\nand a variety of methods to make the model robust and to\\navoid overﬁtting. Our moderation system is trained to detect\\na broad set of categories of undesired content, including sex-\\nual content, hateful content, violence, self-harm, and harass-\\nment. This approach generalizes to a wide range of different\\ncontent taxonomies and can be used to create high-quality\\ncontent classiﬁers that outperform off-the-shelf models.\\n1 Introduction\\nRecent advances in deep learning have accelerated the adop-\\ntion of language models for socioeconomically valuable\\ntasks in the real world (Devlin et al. 2019; Brown et al.\\n2020; Cohen et al. 2022). Both the systems’ builders and its\\nusers may beneﬁt from a responsible deployment approach\\nthat includes moderating the models’ outputs: First, model\\nproviders may want assurances that the models will not pro-\\nduce content that is disallowed by their policies. Second,\\ncustomers of these models sometimes require control over\\ncontent to mitigate the impact of sensitive use cases or to\\nreduce brand risk. A principled, robust, and efﬁcient moder-\\nation solution can track and measure the model inputs and\\noutputs to ensure safety standards. It can also provide ﬁne-\\ngrained control to enable use cases with sensitive needs,\\nsuch as educational applications. We believe that a strong\\nundesired content classiﬁer lays the foundation for build-\\ning safer AI systems in the wild, as it enables the capacity\\nof moderating, evaluating, and guiding the models towards\\nsafer behavior.\\nExisting work on content detection either focuses mainly\\non a limited set of categories, including toxicity (Pavlopou-\\nlos et al. 2020; Gehman et al. 2020), hate speech (Kwok and\\nWang 2013; Davidson et al. 2017), and abusive content (No-\\nbata et al. 2016; Vidgen et al. 2019); or is tailored towards a\\ntargeted use case, such as Perspective API (Jigsaw) on online\\nCopyright © 2023, Association for the Advancement of Artiﬁcial\\nIntelligence (www.aaai.org). All rights reserved.\\n*These authors contributed equally to this work\\nFigure 1: Overview of the model training framework.\\ntoxic comment moderation. There is increasing attention to\\nunderstanding the risk areas of large language models via\\na more rigorous taxonomy (Weidinger et al. 2021), but the\\namount of work is still limited, especially when it comes\\nto deploying language models for real-world applications.\\nHere we build a more comprehensive system for detecting a\\nbroad set of categories of undesired content, including sex-\\nual content, hateful content, violence, self-harm, and harass-\\nment, as well as severe subcategories under each top-level\\ncategory. Large-scale content moderation systems and tool-\\ning exist on a number of platforms (YouTube 2019; Reddit\\n2022). We aim to provide a blueprint for creating such sys-\\ntems across a wide variety of use cases.\\nDetecting undesired content is difﬁcult due to several\\nchallenges. First, there is not a clearly and widely agreed-\\nupon categorization of undesired content. Designing a de-\\ntailed taxonomy for undesired content and operationalizing\\nit for labeling purposes require a lot of work. The categoriza-\\ntion framework usually needs to clarify a signiﬁcant number\\nof corner cases to achieve high inter-rater agreement dur-\\ning labeling. This is further complicated by the subjectivity\\nof some labeling decisions, due to the different social and\\ncultural backgrounds of human annotators. Second, a prac-arXiv:2208.03274v2  [cs.CL]  14 Feb 2023 tical moderation system needs to process real-world trafﬁc.\\nThus a model bootstrapped from public data or academic\\ndatasets would not work well because there exists a big\\ndata distribution shift and taxonomy misalignment. Third,\\nit is rare to encounter certain categories of undesired content\\nin real-world settings. For example, among sampled user\\nprompts we observed that only 0.04% of cases included self-\\nharm and 0.017% included hateful content involving threats.\\nHence, we need smart solutions to the cold start problem and\\neffective ways to discover undesired samples.\\nMultiple components contribute to the success of building\\nand deploying a practical, general moderation system into\\nthe real world. These include effectively establishing a chain\\nof carefully polished and curated conﬁgurations for data col-\\nlection, data labeling, model training and active learning.\\nBased on our experimentation, we ﬁnd the following con-\\nclusions to be especially noteworthy.\\n•Detailed instructions and quality control are needed to\\nensure data quality. Labeling instructions that lack suf-\\nﬁcient precision force annotators to rely on their sub-\\njective judgment, resulting in inconsistently labeled data\\nthat confuses the model. Regular calibration sessions are\\nnecessary to reﬁne these instructions and ensure annota-\\ntors are aligned with them. And a poorly chosen qual-\\nity metric can lead to data that hurts model performance.\\n(See §3.2)\\n•Active learning is a necessity. There is likely a large dis-\\ntribution shift between public data and the trafﬁc from\\none’s production system. Thus, it is critical to collect\\nnew training samples from the production trafﬁc. Ac-\\ntive learning can effectively expand the training dataset\\nto capture a signiﬁcantly (up to 22 \\x02) larger amount of\\nundesired samples when dealing with rare events. This\\ncan lead to a performance improvement in the underly-\\ning model of up to 10 \\x02for rare categories. (See §3.1 and\\n§4.3)\\n•Use public datasets with care. Publicly available data\\nmight not lead to high quality performance for the prob-\\nlem in hand due to differences in taxonomy and training\\ndata distribution, but can be used to construct a noisy cold\\nstart dataset at the early stage. However, adding academic\\ndata into the training set may hurt the model performance\\nat a later stage when there are enough properly labeled\\ndata samples. (See 3.5)\\n•Imbalanced training data can lead to incorrect gen-\\neralization. Deep learning models could easily overﬁt\\ncommon phrases or templates. For example, the model\\ncan over-generalize to anything formatted as \"X is\\nhateful\" if the data distribution is off-balance. We\\ntackle this challenge by programmatically identifying\\noverﬁtted phrases and by red-teaming via human tri-\\nals. We then alter the training distribution by incorpo-\\nrating model-generated or human-curated synthetic data\\nto patch the weakness. (See §3.5 and §3.3)\\n•Mistakes in data will happen and need to be managed.\\nEven with signiﬁcant amounts of effort on data qual-\\nity control, we still run into mislabeled examples. We\\nexplore different methods for identifying those cases,including cross-validation and hunting down common\\nphrases causing the model to overﬁt via token subtrac-\\ntion. (See §3.2 and §3.5)\\nWe aim to present a holistic approach to building a reli-\\nable and robust undesired content detection model for real-\\nworld applications. Our approach is meant to handle the sce-\\nnario in which the type of the content to be detected is rarely\\nobserved in the real world. We hope that the lessons we\\nlearned are useful to others working on similar problems.\\nWe release a dataset1containing text samples from publicly\\navailable data labeled according to our taxonomy. The model\\nis accessible via the OpenAI Moderation endpoint2.\\n2 Taxonomy\\nGiven that the meaning of a sentence can depend on con-\\ntext, designing a universal taxonomy for undesired content\\nis challenging and requires inherent trade-offs. Throughout\\nthis exercise, we aimed to make the dataset as broadly appli-\\ncable as possible to maximize its utility across research and\\nindustrial contexts, but recognize it still ought to be critically\\nevaluated prior to use in any context, The anatomy of what’s\\nundesired can vary based on factors like use cases, users, re-\\ngions and many others. Some sentences labeled as undesired\\naccording to our taxonomy (e.g. \"I will kill you\"\\nwould be labeled as containing violence) may not be con-\\nsidered undesired when appearing in another context such\\nas ﬁction.\\nAdditionally, we aimed to capture layers of subcategoriza-\\ntion and granularity. Much existing work lacks subcatego-\\nrizations to capture a spectrum of severity, which is critical,\\nsince statements such as \"kill all [IDENT GROUP]\"\\ncan have different repercussions compared to \"[IDENT -\\nGROUP] are dishonest\" . The existence of subcate-\\ngories can also largely reduce the disagreement rate and cor-\\nner cases during labeling, making it valuable for data quality\\ncontrol. In the following outline, we present our taxonomy\\nfor undesired content categorization, including ﬁve top-level\\ncategories and a spectrum of subcategories.\\nS: Sexual content Content that depicts explicit or implicit\\nsexual behavior. Subcategories that are considered unde-\\nsired: [ S3] sexual content involving minors, [ S2] content\\nthat depicts certain sexual activities which could be ille-\\ngal3if they happened in real life today, and [ S1] erotic\\nsexual content that does not depict illegal activities. Sub-\\ncategories that are not considered undesired: [ S0] Non-\\nerotic or contextualized sexual content, such as medical\\nor sex education material.\\nH: Hateful content Content that is threatening, insulting,\\nderogatory and otherwise abusive content targeting spe-\\nciﬁc chosen groups or members of the group because of\\ntheir group identities. Subcategories that are considered\\n1https://github.com/openai/moderation-api-release; sourced\\nfrom CommonCrawl and model-generated data\\n2https://beta.openai.com/docs/guides/moderation; Harassment\\ncategory is currently under further improvement and will be avail-\\nable in the future.\\n3This mapped most closely to what’s illegal in USA. undesired: [ H2] hateful content calling for violence or\\nthreatening, [ H1] derogatory stereotypes or support for\\nhateful statements. Subcategories that are not considered\\nundesired: [ H0.a ] neutral statement referring to group\\nidentity and [ H0.b ] contextualized hate speech, such as\\na quote of other individual’s statement in order to provide\\ncommentary.\\nV: Violence Content that depicts or shows support for phys-\\nical violence. Subcategories that are considered unde-\\nsired: [ V2] extremely graphic violence, [ V1] threats or\\nsupport for violence. Subcategories that are not consid-\\nered undesired: [ V0] neutral depictions of contextualized\\nviolence.\\nSH: Self-harm Content that contains a graphic description\\nor shows support for self-harm.\\nHR: Harassment Content that may be used to torment or\\nannoy individuals in real life, or make harassment more\\nlikely to occur.\\nOur model learns to predict whether a given sample vio-\\nlates any of 8 chosen categories , including all the top cate-\\ngories ( S,H,V,SH,HR) and three most severe subcategories\\n(S3,H2, andV2).\\n3 Methods\\n3.1 Data Selection and Active Learning\\nTo ensure that our moderation system performs well in the\\ncontext of our production use cases, we need to incorporate\\nproduction data to our training set. We set up a three-stage\\nprocedure in an iterative fashion.\\nFirst, a large volume of our production data is selected\\nat random. Any potential personally identiﬁable information\\n(PII) is masked. The most recent moderation model is used\\nto score these samples and discover which ones may trigger\\nany chosen categories.\\nIn the second stage we run a simple active learning strat-\\negy to select a subset of most valuable samples to be labeled\\nout of the random samples extracted in stage one. The ac-\\ntive learning strategy is composed of three parallel pipelines.\\nThe ﬁrst one relies on random sampling such that some frac-\\ntion of our data remain consistent with the underlying data\\ndistribution in production. The second one randomly selects\\nfrom samples with model score above a certain threshold\\nfor each category to identify likely undesired data points.\\nThe last pipeline adopts a set of uncertainty sampling strate-\\ngies (Lewis and Gale 1994; Lewis and Catlett 1994) to cap-\\nture samples that the model is most uncertain about, where\\nthe model score for that category is closest to 0.5.\\nDuring the ﬁnal stage, all the samples selected by differ-\\nent active learning strategies are aggregated and re-weighted\\nbased on statistics of certain metadata associated with it.\\nThe sampling weight is conﬁgured to be proportional to\\nthe square root of the sample count. This helps improve\\nthe diversity of selected samples with regard to the associ-\\nated metadata. We update the sub-strategy mixture over time\\nbased on changes in the data distribution and categories that\\nwe want to improve the most at different stages.3.2 Labeling and Quality Control\\nData label correctness is critical to good model performance.\\nGetting such data can be difﬁcult given that our categories\\nand the boundary lines between them are inherently sub-\\njective. However, certain interventions can signiﬁcantly im-\\nprove the quality of labeled data.\\nOne important intervention for improving data quality\\n- in terms of both consistent labels across different anno-\\ntators as well as between annotators and researchers - is\\nto make the labeling instructions as well-deﬁned andcon-\\ncrete as possible. To make the instructions well-deﬁned, we\\nsought to design detailed deﬁnitions and design categories\\nor subcategories to be as mutually exclusive as possible so\\nas to minimize ambiguity. To make the instructions concrete,\\nwe hosted regular calibration sessions to review ambiguous\\nedge cases and instances where external annotators and our\\ninternal auditors disagree. Based on feedback from those\\nsessions, we made the instructions more clear and concrete,\\nwith numerous examples and clearer deﬁnitions around bor-\\nderline cases. As rules are deﬁned clearly and concretely to\\nminimize subjective judgments, they can be executed more\\nconsistently by the annotators.\\nRegular, ongoing audits are necessary to ensure that la-\\nbeled data continues to be of sufﬁciently high quality. The\\nchoice of which samples to audit and what metrics to use\\nto measure data quality is crucial. We found that selecting\\nauditing targets at random cannot maximize the value out\\nof auditing due to the imbalanced distribution across cate-\\ngories. The annotator-auditor agreement rate (i.e. accuracy)\\nis suboptimal because undesired examples are rare events\\nto encounter and the accuracy can be arbitrarily high due\\nto the abundance of true negatives. Instead, in each cho-\\nsen category, we randomly select 10 samples labeled as\\nundesired and 10 samples with model probability greater\\nthan 50%. The former help capture false positive cases and\\nthe latter provide an estimation on recall. Then we com-\\npute the F-1 score for the chosen samples based on the\\nannotator-assigned labels while using auditor-assigned la-\\nbels as ground truth. This procedure performs much better\\nin practice when certain categories of undesired data points\\nare rare. Separation of metrics per category makes it easy to\\nrecognize category-speciﬁc issues and to retrain annotators\\naccordingly.\\nEven with very clear labeling instructions and an effec-\\ntive audit procedure, mistakes in data are still unavoidable.\\nTo identify potentially mislabeled samples in our dataset,\\nwe periodically split our current training dataset into two\\nparts, train separate models on those datasets and use each\\nmodel to score another half of the dataset that model was\\nnot trained on. When the model prediction disagrees with\\nthe current ground-truth label, the sample in question gets\\nﬂagged. A random portion of ﬂagged samples is audited, and\\nif more than 30% are identiﬁed as mislabeled, all ﬂagged\\nsamples would get labeled again for the second time.\\n3.3 Synthetic Data\\nIn addition to the data collection discussed above, we also\\nuse synthetic data to improve model performance on rare Example prompt\\n>>The text is about self harm, more speciﬁcally: physical self-harm (cutting).\\n>>The narrator is third person (a friend).\\n>>The narrator is a young teenager (female).\\n>>The act of self harm happens: last week.\\n>>Type of text: online forum post.\\n>>The following word should come up in the text: camera.\\n>>The text should not contain the phrase ”self harm” or ”self-harm”.\\n>>The writing level is: average, some errors.\\n>>The text starts below:\\nTable 1: Example zero-shot prompt template for generating\\nsynthetic SHdata. The sections in green are ﬁlled with ran-\\ndom ingredients to encourage diversity.\\ncategories such as SHand to mitigate the counterfactual bias\\ntowards certain demographic attributes (Kusner et al. 2017;\\nGarg et al. 2019; Dwork et al. 2012). Generating synthetic\\ndata through large pre-trained language models has shown\\nto be an effective way for data augmentation (Anaby-Tavor\\net al. 2020; Kumar, Choudhary, and Cho 2020; Yoo et al.\\n2021) and it is particularly helpful when there is little to no\\ninitial data (“cold start”) or when there are not enough un-\\ndesired samples in the production trafﬁc.\\nZero-shot data for cold start. To kick start the active\\nlearning and labeling process, we need some initial data to\\nbuild the ﬁrst version of the model and train annotators.\\nHowever, it is difﬁcult to ﬁnd existing public datasets on\\ncertain categories such as SHandV2. We tackle the prob-\\nlem by generating a synthetic dataset with zero-shot prompts\\non GPT-3. The prompts are constructed from human-crafted\\ntemplates and we label the generated texts as the initial\\ndataset. Table 1 provides an example prompt for SH.\\nFew-shot data for rare categories. Some sub-categories\\nhad minimal amounts of undesired data even after several\\niterations of active learning. To address this, we constructed\\nfew-shot prompts with existing undesired examples and sent\\nthe generated texts to be labeled. The generated texts are\\nmanually inspected to avoid bias ampliﬁcation (Zhao et al.\\n2017). We observed a nontrivial performance improvement\\nby incorporating the synthetic dataset.\\nCurated data to mitigate counterfactual bias. Similar\\nto other existing NLP models, our models also suffer from\\ncounterfactual bias towards certain demographic attributes\\nas bias commonly exists in the training data. For instance,\\n\"black women.\" was classiﬁed as hateful content with\\nhigh conﬁdence in earlier versions of the model. We mitigate\\nthe issue by curating a synthetic dataset with templates that\\ntend to lead to hateful predictions, e.g., \"[subject] is\\nselfish/foolish/narrow-minded.\" . The [sub-\\nject] could either be ﬁlled with real demographic at-\\ntributes (e.g., Latino ) or random object names (e.g.,\\n\"black blanket\" ), which forms hateful and safe sam-\\nples respectively. We observe that the curated dataset not\\nonly mitigates bias to some degree, but also helps improve\\nthe model performance. For instance, the average AUPRC\\non hateful content was improved from 0:417 to0:551 by\\nadding 69k curated synthetic examples. We believe this is\\nbecause the contrastive setup of subjects in synthetic exam-ple templates encourages the model to infer the correct fea-\\nture representations: negative descriptive words or individ-\\nual identity groups alone are not enough to be considered\\nhateful, and only when they appear together they might be\\nconsidered hateful. Despite the observed improvements, the\\nsynthetic dataset also has limitations and we will continue\\nimproving it in the future (§6).\\nLarge amount of noisy data does not help. To under-\\nstand whether it is helpful to include a large amount of noisy\\nsynthetic data, we also generated zero-shot and few-shot ex-\\namples twice the size of the existing labeled training dataset.\\nFor zero-shot examples, we set the label to positive or neg-\\native if the prompt asks the model to generate undesired or\\nsafe examples, respectively. For few-shot examples, we set\\nthe label to positive or negative if all of the few-shot exam-\\nples are undesired or safe, respectively. Contrary to previous\\nstudies (Wang et al. 2021b; Schick and Sch ¨utze 2021), we\\nfound mixing noisy synthetic data into training hurt model\\nperformance. It is worth noting that many existing stud-\\nies on synthetic data usage experimented in the no-to-low\\ndata regime, where only a handful of labels are available.\\nHowever, in our experiment, we have collected a large high-\\nquality dataset and we suspect that noise introduced by syn-\\nthetic data confuses the model and lowers the learning efﬁ-\\nciency.\\n3.4 Domain Adversarial Training\\nWe intended to make good use of existing public NLP\\ndatasets to improve the performance of our models. How-\\never, we observed that models trained on public NLP\\ndatasets do not perform well on our production trafﬁc. This\\nis likely due to the distribution difference between domains.\\nFor instance, examples from our production trafﬁc are usu-\\nally much longer and contain few-shot prompts, whereas\\nexisting public NLP datasets are usually shorter and often\\ncrawled from Wikipedia, Twitter, etc. (Vidgen and Derczyn-\\nski 2020). To mitigate the problem, besides carefully tuning\\nthe mixture of public datasets and production data, we in\\naddition apply Wasserstein Distance Guided Domain Adver-\\nsarial Training (WDAT) to encourage the model to learn do-\\nmain invariant representations (Arjovsky, Chintala, and Bot-\\ntou 2017; Ganin et al. 2016).\\nWe follow Shen et al. (2018) and approximate the Wasser-\\nstein distance by maximizing the loss of a domain critic\\nhead. Letfz(x) :Rd!Rzbe the feature extractor that\\nmaps thed-dimensional input into a z-dimensional embed-\\nding,fc(h) :Rz!Rcbe a multiclass classiﬁcation head,\\nandfd(h) :Rz!Rbe the domain critic head that maps\\nthe embedding into real number. The domain critic loss is\\ndeﬁned as\\nLd(Ds;Dt) =jE\\nx2Dsfd(fz(x))\\x00E\\nx2Dtfd(fz(x))j:\\nCombined with the regular classiﬁcation loss Lc, our objec-\\ntive is to solve the following minimax problem:\\nmin\\n\\x12z;\\x12cfLc+\\x15max\\n\\x12dLdg; where\\x12z;\\x12c;\\x12dare the parameters of fz;fc;fd, respec-\\ntively. Our model uses a transformer encoder as the feature\\nextractorfz.\\nIn our implementation, we use the absolute value in Ld\\nsince the initial loss could be negative, and clip \\x12din a com-\\npact space [\\x000:01;0:01]to enforce the Lipchitz constraint.\\nWe empirically set the balancing coefﬁcient \\x15to 0.01. In ex-\\nperiments, WDAT achieves a more stable training compared\\nto the original classiﬁer-based approach (Arjovsky, Chintala,\\nand Bottou 2017), and yields better performance on our pro-\\nduction trafﬁc with and without labeled production data in\\nthe training set.\\n3.5 Model Probing\\nIt is widely known that ML models are vulnerable to ad-\\nversarial inputs and may make predictions based on seem-\\ningly irrelevant features (Szegedy et al. 2013; Goodfellow,\\nShlens, and Szegedy 2015; Alzantot et al. 2018; Zhang et al.\\n2021). For instance, a sentiment classiﬁcation model may\\nmake different predictions for \"a short and moving\\nfilm.\" and\"a short and moving movie.\" , in-\\ndicating that the model makes predictions based on film/-\\nmovie rather than descriptive words. To make sure that our\\nmodel performs as expected, we conduct tool-assisted model\\nprobing and human-in-the-loop red-teaming.\\nKey Tokens Probing. Our ﬁrst goal is to verify whether\\nthe model is making predictions based on correct features.\\nFor instance, we observed that our models may incorrectly\\npredict \"all immigrants.\" ashate , which is pos-\\nsibly because among 21 training examples containing the\\nphrase (from R ¨ottger et al. (2021)), 17 examples are unde-\\nsired and only 4 examples are safe. Table 2 provides a few\\nexamples, where the expected behavior is to also take k1ll\\nandD3ath into account. To identify such potentially over-\\nﬁtted key tokens, we apply input reduction (Feng et al. 2018)\\non the training dataset by removing tokens greedily until the\\nprediction drops below 0.8. On average, we were able to\\nreduce the number of characters per sample from 722.3 to\\n15.9. By cross-validating with third-party APIs and heuris-\\ntic keywords, we infer that about 97% of the identiﬁed key\\ntokens are indeed unsafe, indicating that the model is pick-\\ning up the correct feature most of the time. For the remaining\\n3% key tokens, we manually reviewed them and discovered\\nthe following problems:\\n1. The frequency of irrelevant symbols is high (e.g., the pe-\\nriod symbol contributes to the prediction score).\\n2. Some key tokens are long and close to the original exam-\\nple, hinting that the model memorizes training samples.\\n3. Our model is not strong at handling typos and adversarial\\ninputs (e.g., \"w h o r e s\" becomes \"e s\" ).\\nWe partially addressed the above issues, and plan to re-\\nsolve with better data augmentation in the future (§6).\\nHuman Red-teaming. As a ﬁnal layer of the veriﬁcation\\nprocess, we conduct internal red-teaming before releasing\\nnew models. The goal of the red-teaming is to uncover any\\nunexpected model weaknesses beyond the test dataset. Dur-\\ning the process, we discovered the following issues:Hate Example\\n1 I want to k1ll all immigrants.\\n1 Why is it that all immigrants are repulsive?\\n1 D3ath to all immigrants.\\n0 Threatening to kill all immigrants will get\\nyou thrown in jail.\\n:::\\nTable 2: Example training data from (R ¨ottger et al. 2021)\\ncontaining the phrase \"all immigrants.\" The model\\ntrained on it directly may make predictions based on incor-\\nrect features.\\n1. The model predicts high hate scores for examples con-\\ntaining the “#” token. This phenomenon is likely caused\\nby the fact that we have many hate training exam-\\nples from some academic datasets that contain only short\\ntweets.\\n2. The model predicts high hate and violence\\nscores for examples containing potential racial to-\\nkens such as black . It is expected to classify \"I\\nhate black people!\" ashate but not \"I hate\\nblack cats!\" ashate .\\nTo mitigate the above issues, we construct synthetic\\ndatasets from hand-curated templates and synthetic model\\ngenerations to patch the holes (§3.3), and adjust the train-\\ning dataset distribution to make sure we have the right\\nmix across multiple types of text sourced from academic\\ndatasets. The process can be iterative, helping us discover\\nnew issues and solutions in each round and naturally lead-\\ning to improved robustness and consistency in time when the\\nred-teaming process can be executed more regularly and at\\nscale.\\n4 Experiment Results\\n4.1 Model Architecture and Training\\nOur model is a lightweight transformer decoder model\\nwhere the ﬁnal output linear layer is replaced with 8 MLP\\nheads, each corresponding to one independent matrix of\\nshape [dmodel;256;1], wheredmodel is the transformer model\\nsize. We ﬁnd this head architecture works better than a single\\ndeep MLP layer with one output vector of 8 dimensions at\\navoiding interference between categories and requires fewer\\nparameters to train.\\nThe model is initialized from a GPT model that is pre-\\ntrained on a large text corpus and then ﬁne-tuned with learn-\\ning rate 0.05, batch size 256, dropout rate 0.1 within MLP\\nheads and up to 3 epochs.\\n4.2 Model Performance\\nOur model is trained and tested on both production and pub-\\nlic data. We are not able to share the test dataset contain-\\ning production trafﬁc for privacy and legal reasons; hence,\\nwe report the model performance on a different test dataset4\\ncontaining only samples from public data, as well as several\\npublicly available datasets on undesired content detection.\\n4https://github.com/openai/moderation-api-release Perspective Ours\\nPublic S .8709* .9703\\nH .6914 .7968\\nV .5201 .7371\\nHR .3902* .6191\\nSH - .8070\\nS3 - .7638\\nH2 - .7268\\nV2 - .6061\\nJigsaw Identity-hate .6644 .6890\\nInsult .8814 .8548\\nObscene .9500 .8353*\\nThreat .7492 .6144*\\nToxic .9769 .9304*\\nTweetEval Hate .5961 .6473\\nOffensive .7919* .7024*\\nStormfront Hate .8754 .9053\\nReddit Sexual .8961* .9417*\\nTable 3: Comparison of our model with Perspective API on\\nAUPRC (Area under the Precision-Recall Curve) across a\\nset of test datasets. Numbers followed with ”*” are based on\\napproximated taxonomy match, so not an exact fair compar-\\nison.\\nTable 3 compares the performance of our model with\\nPerspective API5as a baseline on our test dataset,\\nTweetEval (Barbieri et al. 2020), Stormfront hate speech\\ndataset (de Gibert et al. 2018), a subset of Reddit com-\\nments with noisy labels on erotic content processed accord-\\ning to Barrientos et al. (2020) and a downsampled Jigsaw\\ntoxic comments test dataset (Jigsaw 2018). None of the\\ntraining portion of external evaluation benchmarks are incor-\\nporated into our training, except for half of Jigsaw’s training\\ndata that has no overlap with the Jigsaw test set in evaluation.\\nUnfortunately, due to the taxonomy mismatch, we cannot\\nhave exact comparison across all categories. For example,\\nour taxonomy does not cover “toxic” and Perspective API\\ndoes not explicitly detect “self-harm” or “sexual content”.\\nSee the details on how we match two taxonomies and pre-\\nprocess each test dataset in Appendix. A.\\nIt is not surprising that our model performs the best on the\\ntest dataset labeled with the same taxonomy and the Perspec-\\ntive API does a better job on Jigsaw data. It further proves\\nthe point on how important it is to align the taxonomy be-\\ntween training data and use cases in evaluation. Our model\\noutperforms the Perspective API baseline on both TweetEval\\nand Stormfront test sets for detecting hateful content, despite\\nthe fact that neither are in the training set.\\n4.3 Active Learning Experiments\\nTo assess the importance of active learning, we evaluate the\\nperformance of our active learning strategy, as described in\\n5https://www.perspectiveapi.com/Category Random\\nSamplingActive\\nLearningMultiplier\\nS 1.49% 25.53% 17.1 \\x02\\nH 0.17% 3.09% 18.2 \\x02\\nV 0.48% 9.92% 20.7 \\x02\\nHR 0.55% 6.41% 11.7 \\x02\\nSH 0.09% 1.85% 20.6 \\x02\\nS3 0.24% 2.42% 10.1 \\x02\\nH2 0.03% 0.67% 22.3 \\x02\\nV2 0.25% 4.27% 17.1 \\x02\\nSafe 96.57% 59.54% -\\nTable 4: Label distributions for samples selected by random\\nsampling and active learning sampling. Note that one sample\\ncan be assigned with multiple labels so the percentages sum\\nup to more than 100%.\\n§3.1, compared to random sampling.\\nIterative training. We run the following training proce-\\ndure twice, using our active learning strategy and random\\nsampling, respectively.\\n1. Start with an initial training dataset D0ofk0= 6000\\nlabeled examples from public data and a validation set V\\nof about 5500 samples from the production trafﬁc.\\n2. fori 0toN\\x001do (N= 3):\\n(a) Train a new model MionDi;\\n(b) Evaluate MionV;\\n(c) Score 5\\x02105production samples with Mifrom our\\nproduction trafﬁc;\\n(d) Choose about 2000 samples from the above data pool\\nvia the selection strategy in test and add samples to the\\ntraining set to construct Di+1after labeling.\\nResults. Table 4 demonstrates the label distributions ob-\\ntained by the two strategies and our active learning strategy\\ncan capture undesired content 10+ times more effectively\\nthan random sampling on all categories. Overall about 40%\\nof samples selected by active learning can trigger at least one\\nundesired label, while in comparison only 3.4% of random\\nsamples are assigned with any undesired label.\\nAs shown in Fig. 2, using the active learning strategy to\\ndecide which new data samples leads to a greater improve-\\nment across all categories than random sampling. We ob-\\nserve signiﬁcant performance improvement on all categories\\nwith active learning after 3 iterations.\\n4.4 Domain Adversarial Training Experiments\\nWe want to understand the effectiveness of Wasserstein Dis-\\ntance Guided Domain Adversarial Training (WDAT) under\\nthree scenarios: (1) At the beginning of the project, we only\\nhave labeled public data and unlabeled production data. (2)\\nIn the middle of the project, we also curate synthetic exam-\\nples to improve model weaknesses. (3) At the later stage,\\nwe get a sufﬁcient amount of labeled production examples.\\nAll three circumstances are important because we want to Figure 2: Performance of active learning sampling versus\\nrandom sampling on the same validation set at each model\\niteration, measured by AUPRC.\\nmake good use of unlabeled production data to train the best\\nmodel throughout the project, and a strong model on pro-\\nduction trafﬁc boosts the effectiveness of active learning at\\nevery iteration. We use the following setup to compare the\\nperformance on our production trafﬁc.\\nDatasets. We create three training datasets PUB, SYN,\\nand MIX to study (1), (2), and (3), respectively. PUB con-\\nsists of around 90k public examples including both samples\\nfrom academic datasets and Web data (Common Crawl) la-\\nbeled by our annotators. SYN adds additional 69k curated\\nsynthetic examples. MIX contains all examples in SYN with\\nadditional 60k production samples with labels.\\nModels. The baseline models are trained with basic super-\\nvised learning. The DAT models are trained with two hidden\\nlayers of 300 dimensions using additional 100k unlabeled\\nproduction data points. All models are trained with up to 2\\nepochs, and the training is repeated 3 times with different\\nrandom seeds.\\nResults. We compare the average AUPRC on the produc-\\ntion validation set V. As demonstrated in Table 5, the im-\\nprovement from WDAT is signiﬁcant when we only haveCategory PUB SYN MIX\\nBaseline DAT Baseline DAT Baseline DAT\\nS .698 .730 .726 .745 .943 .939\\nH .417 .491 .551 .476 .843 .818\\nV .490 .529 .532 .531 .640 .633\\nHR .258 .369 .326 .356 .453 .482\\nSH .063 .281 .086 .296 .621 .632\\nS3 .592 .759 .779 .777 .911 .936\\nH2 .393 .643 .570 .577 .851 .854\\nV2 .165 .453 .093 .507 .443 .533\\nTable 5: The average AUPRC on a production validation\\nset. PUB denotes models trained on labeled public datasets,\\nSYN adds additional synthetic examples, and MIX adds ad-\\nditional labeled production examples. We mark the best re-\\nsult within each conﬁguration in bold .\\naccess to public datasets (PUB), and the marginal gain re-\\nduces gradually as we add more training examples, espe-\\ncially in-distribution production samples. For instance, DAT\\nimproved SHAUPRC from 0.063 to 0.281 on PUB and from\\n0.086 to 0.296 on SYN, whereas the improvement is only\\nfrom 0.621 to 0.632 on MIX. WDAT still helps weak cate-\\ngories ( SHandV2) on SYN and MIX, but it may slightly\\nhurt the performance for categories with a sufﬁcient amount\\nof in-distribution data such as HandV. We suspect this is\\nbecause the model failed to ﬁnd a representation that works\\nvery well for both the public datasets and our production dis-\\ntribution. Further study on the model architecture and train-\\ning methods is required to improve the performance on all\\ncategories with unlabeled data throughout different stages of\\nthe project.\\n5 Related Work\\nThere is a long track record of work on the deﬁnition\\nand detection of hateful, toxic, offensive and abusive con-\\ntent (Kwok and Wang 2013; Nobata et al. 2016; Waseem\\n2016; de Gibert et al. 2018; Vidgen et al. 2019; Gehman\\net al. 2020; Rosenthal et al. 2020; Lees et al. 2022). Zampieri\\net al. (2019) proposed a three-level hierarchical taxonomy\\nconsidering whether the given language is (i) offensive or\\nnot; (ii) targeted or not; and (iii) targeted at a group, an\\nindividual or other organizations. Usually hateful expres-\\nsions targeting protected identity groups are considered hate\\nspeech (Davidson et al. 2017). Perspective API deﬁnes toxi-\\ncity as ”A rude, disrespectful, or unreasonable comment that\\nis likely to make people leave a discussion”. Some also used\\ntoxicity as a general umbrella term for offensive, abusive,\\nand hateful language (Pavlopoulos et al. 2020). The deﬁni-\\ntions of hatefulness, toxicity, offensiveness and abusiveness\\nhave overlaps but are not exactly the same, creating obsta-\\ncles for sharing datasets between projects. Furthermore, only\\na limited amount of work considered detailed subcategoriza-\\ntions (Mollas et al. 2020; Borkan et al. 2019) to capture a\\nspectrum of severity, making it harder to control labeling\\nquality. Finally, there exist various types of potentially un-\\ndesired text in the wild, such as sexual content involving mi-\\nnors, extreme graphic violence, or support for self-harm or\\nsuicides, besides offensive and abusive language, and we ob- served a gap between current research work and the entirety\\nof content types that should be moderated and detected. Our\\nwork aims to ﬁll in the gap.\\nDespite the common belief that training data quality is\\ncritical for model performance, there is still lack of com-\\nmunity standards for labeling standards, annotator training,\\nquality metrics, etc. (Vidgen and Derczynski 2020; Yin and\\nZubiaga 2021; Lees et al. 2022; PAI 2021). Vidgen and Der-\\nczynski (2020) studied 60+ datasets for abusive language\\ndetection and found that the primary data source is Twit-\\nter and expert coding is the most common way to anno-\\ntate data, closely followed by crowdsourcing. For large-scale\\ndata collection, crowdsourcing remains the most common\\napproach (Mollas et al. 2020; Zampieri et al. 2019; Davidson\\net al. 2017). However, the weak skill set of non-expert anno-\\ntators can lead to lower data quality (Waseem 2016; Yin and\\nZubiaga 2021). Some recent work turns to large pre-trained\\nlanguage models to generate synthetic data, signiﬁcantly re-\\nducing the cost of time and human labor (Wang et al. 2021a;\\nHartvigsen et al. 2022), but it is unclear whether model out-\\nputs would be diverse enough to adapt to the real-world dis-\\ntribution. Synthetic data can be hand-crafted (R ¨ottger et al.\\n2021), but it is limited by size and thus more suitable for\\nevaluation. It is noteworthy that training data can contain\\nbias due to the subjectivity and biases in the data collec-\\ntion process (Davidson, Bhattacharya, and Weber 2019; Sap\\net al. 2019).\\nActive learning has been successfully applied to a num-\\nber of different domains such as text classiﬁcation (Lewis\\nand Gale 1994; Schohn and Cohn 2000; Siddhant and Lipton\\n2018); machine translation (Zeng et al. 2019); image classi-\\nﬁcation (Luo et al. 2005; Hoi et al. 2006; Gal, Islam, and\\nGhahramani 2017); object detection (Schmidt et al. 2020)\\nand information retrieval (Shen and Zhai 2005). There are\\nseveral families of active learning sampling strategies that\\nare often used in practice. Uncertainty sampling selects data\\npoints about which the model is most uncertain. The uncer-\\ntainty of the model can be quantiﬁed by predicted proba-\\nbilities (Lewis and Gale 1994; Lewis and Catlett 1994; Cu-\\nlotta and McCallum 2005; Scheffer, Decomain, and Wro-\\nbel 2001), disagreement among an ensemble of models (Se-\\nung, Opper, and Sompolinsky 1992; Dagan and Engelson\\n1995; McCallum and Nigam 1998), or by using dropout and\\nBayesian approaches (Gal, Islam, and Ghahramani 2017;\\nSiddhant and Lipton 2018). Diversity sampling chooses\\nsamples in a way that ensures sufﬁcient diversity within\\nthe selection. This is commonly achieved by clustering un-\\nlabeled data and sampling from different clusters (Nguyen\\nand Smeulders 2004; Xu, Akella, and Zhang 2007), or by\\nselecting samples which are ”representative” of the sample\\ndistribution (i.e., which are similar to many other samples)\\n(McCallum and Nigam 1998; Settles and Craven 2008). Un-\\ncertainty and diversity sampling are sometimes combined in\\na single complex active learning strategy.\\nRed-teaming is a common approach for model improve-\\nment by discovering and patching the weakness itera-\\ntively (Dinan et al. 2019; Vidgen et al. 2020; Kiela et al.\\n2021; Ziegler et al. 2022; Perez et al. 2022; Ribeiro et al.\\n2020), where humans are encouraged to look for examplesthat could fail the model. Dynabench (Kiela et al. 2021)\\nis built as a platform for easy adversarial data collection.\\nMishkin et al. (2022) describes in detail an operational pro-\\ncess for doing red-teaming using external experts. Ziegler\\net al. (2022) designed a tool to efﬁciently assist human ad-\\nversaries to identify failures in a classiﬁer. Models trained\\nwith red-teaming data are found to be more robust to ad-\\nversarial attack (Dinan et al. 2019; Ziegler et al. 2022) and\\nhuman-in-the-loop dynamic data collection can efﬁciently\\nimprove model performance (Kiela et al. 2021; Vidgen et al.\\n2020).\\nDomain adaptation aims at generalizing knowledge\\nlearned in the source domain towards a related target do-\\nmain (Ben-David et al. 2006; Weiss, Khoshgoftaar, and\\nWang 2016; Ben-David et al. 2009), the technique is most\\nuseful when there is insufﬁcient labeled data in the target\\ndomain but sufﬁcient labeled data in the source domain. Dif-\\nferent methods have been proposed to transfer the knowl-\\nedge across domains (Ramponi and Plank 2020; Blitzer,\\nMcDonald, and Pereira 2006; Mansour, Mohri, and Ros-\\ntamizadeh 2008). Inspired by generative adversarial nets\\n(GANs) (Goodfellow et al. 2014) which train a discrimina-\\ntor to make the representations of source and target indistin-\\nguishable, Domain Adversarial Training (DAT) methods are\\nproposed to reduce the domain discrepancy through a do-\\nmain discriminator (Arjovsky, Chintala, and Bottou 2017;\\nGanin et al. 2016; Tzeng et al. 2017; Ganin and Lempitsky\\n2015). To learn domain-invariant feature representations,\\nDAT employs a gradient reversal layer to maximize the min-\\nimal loss of the domain discriminator. However, DAT suffers\\nfrom a gradient vanishing problem when the domain dis-\\ncriminator can tell apart the two domains easily, and Wasser-\\nstein distance based methods are proposed to enable a more\\nstable training (Shen et al. 2018; Arjovsky, Chintala, and\\nBottou 2017; Shah et al. 2018).\\n6 Future Work and Limitations\\nBias and Fairness. Similar to other existing NLP mod-\\nels, our models also suffer from bias towards certain de-\\nmographic attributes (Kusner et al. 2017; Garg et al. 2019;\\nDwork et al. 2012). For instance, the model may give higher\\nhate predictions if the input contains gay and higher\\nsexual predictions if the input contains her. This is be-\\ncause we use data from the Internet, and social bias may\\npresent explicitly or implicitly in the training datasets. We\\ntried mitigation methods such as creating a balanced syn-\\nthetic dataset with templates but could not fully eliminate\\nthe issue. In the future, we will continue following related\\nresearch and improve the fairness of our models.\\nData Augmentation. We plan to investigate more data\\naugmentation methods to boost the training dataset. Al-\\nthough our current training dataset naturally includes mis-\\nspelled words and incorrect grammar as some of it is user-\\ngenerated content, it is valuable to experiment with data\\naugmentation to improve lexicon robustness (Wei and Zou\\n2019; Kobayashi 2018; Zhang et al. 2021) and the generaliz-\\nability of the model (Guo, Mao, and Zhang 2019; Shen et al.\\n2020; Gao, Yao, and Chen 2021), especially when working with the changing distribution of real-world data.\\nBetter Multilingual Support. Only about 5% of the sam-\\nples are non-English in our training set. As the vast majority\\nof our production trafﬁc is in English, we have not yet rig-\\norously evaluated or optimized performance on non-English\\ntext. Multilingual toxic content classiﬁcation (Aluru et al.\\n2020; Wang and Banko 2021; Lees et al. 2022) would re-\\nquire more non-English training data and may need addi-\\ntional changes on tokenization or model architecture.\\nRed-teaming at scale. Red-teaming is an effective way to\\nﬁnd unknown failure cases for the model. Currently we do\\ninternal red-teaming with each new model version, which is\\nnot a scalable approach. In the future, we plan to set up a\\npipeline for model red-teaming similar to the one we have\\nfor labeling production trafﬁc. We plan to use a specialized\\ninterface inspired by Kiela et al. (2021); Ziegler et al. (2022)\\nto improve the efﬁciency of the red-teamers.\\nMore Active Learning Experiments. Our current active\\nlearning strategy to select high-value data for labeling is\\nquite simple. For example, we did not explore diversity sam-\\npling due to computational restriction. Onward we plan to\\nrun more rigorous experiments comparing the performance\\nof different active learning strategies, as well as more so-\\nphisticated strategies, incorporating both uncertainty and di-\\nversity sampling.\\n7 Broader Impacts\\nContent moderation classiﬁers have many uses. When paired\\nwith fair and robust enforcement practices, they have the\\npotential to reduce certain instances of misuse6by ensur-\\ning that policies are operationalized on both inputs and out-\\nputs of language models. Classiﬁers also enable ﬁltration of\\ndatasets at scale, which may be used to train language mod-\\nels with desired properties (Welbl et al. 2021) and allow for\\nbetter evaluation of language models (Gehman et al. 2020).\\nLonger-term, content moderation classiﬁers can be used as a\\nway to ensure high-stakes reliability in very-capable AI sys-\\ntems (Ziegler et al. 2022)—a critical necessity for enabling\\nthe deployment of those systems in certain domains.\\nWhile this underscores the importance of the undesired\\ncontent classiﬁers, all classiﬁers rest on certain assumptions\\nand decisions that may present vulnerabilities or make them\\ninappropriate for certain use cases or types of text. Addition-\\nally, these tools can suffer from problematic biases, such as\\ndisproportionate false positives when discussing groups that\\nare frequently the target of hate. (Garg et al. 2019)\\nThe following sections discuss the normative and subjec-\\ntive questions on which these classiﬁers rest and explore the\\nchallenges they present.\\n7.1 Challenges of Taxonomy Design\\nWe take care to design our taxonomy to reﬂect generaliz-\\nable viewpoints. However, much of our data is drawn from\\na US-centric context and the taxonomy was designed to best\\n6misuse may be deﬁned as uses of the model that the moderat-\\ning body does not want to allow, e.g. generation of hateful contentﬁt this data. Additionally, while we have designed our tax-\\nonomy to be as comprehensive as possible, it would still\\nbe useful for future researchers to add and update the cat-\\negories based on their own use cases and deployment con-\\ntexts. Given the sensitive nature of various tasks, we also\\nencourage the use of this taxonomy in concert with other\\nmitigation strategies, as there is no silver bullet for content\\nmoderation.\\nWe hope that this work will encourage further discussion\\nand debate around the principles and values that underpin\\ncontent moderation.\\n7.2 Annotator Viewpoints and Disagreement\\nIt is commonly agreed that the annotation of toxic language\\nis subjective and that annotators’ interpretations may be in-\\nﬂuenced by their personal and cultural backgrounds, includ-\\ning lived experiences, values and demographic factors. For\\nexample, Waseem (2016) found that feminist and anti-racist\\nactivists systematically disagree with crowd workers on their\\nhate speech annotations. In their study, agreement between\\nthe authors, amateurs and expert annotators is low (14 %),\\nmost often because in many instances where the authors had\\nidentiﬁed hate speech, annotators do not.\\nBy necessity, incorporating diverse viewpoints invites dis-\\nagreement on annotation labels. Much of the computer sci-\\nence literature focuses on eliminating inter-rater disagree-\\nments, most often via deliberation or majority vote. How-\\never, in the case of data from or about marginalized popu-\\nlations, disagreement may be a meaningful signal: An ad-\\nverse effect of majority vote in such cases is limiting rep-\\nresentation of minority perspectives in data Prabhakaran,\\nMostafazadeh Davani, and Diaz (2021), potentially reinforc-\\ning societal disparities and harms. Moreover, analyzing dis-\\nagreements may lead to a better understanding of the domain\\nof application Patton et al. (2018).\\nIn their study, rather than aggregating, Davani, D ´ıaz,\\nand Prabhakaran (2021) preserve annotator disagreements,\\nwhich they note could reﬂect useful and nuanced informa-\\ntion about the uncertainty of a sample’s membership to a\\nclass. Indeed, they demonstrate that their approach yields\\nthe same or better performance than similar approaches with\\naggregated labels, while retaining the ability to estimate un-\\ncertainty in predictions that correlate with real-life annotator\\ndisagreements.\\nMoreover, resolving disagreement via majority vote may\\nbe at odds with preserving minority opinions in subjective\\ntasks. Ovesdotter Alm (2011) argues that achieving a single\\nreal ”ground truth” label is impossible and is not essential\\nin subjective tasks, and calls for ﬁnding ways to model sub-\\njective interpretations of annotators, rather than seeking to\\nreduce the variability in annotations.\\n7.3 Annotator Selection and Welfare\\nWe are committed to ensuring that our labeling tasks are\\nmanaged in a considerate and ethical manner, and we strive\\nto follow current best practices for sourcing data labeling\\nservices (PAI 2021). Via our data vendors, all of our annota-\\ntors are selected for their skill and willingness to participate\\nin these difﬁcult tasks. Before they opt in, all annotators are vetted by counselors and made aware of the risks and poten-\\ntial harms of working with sensitive data. Our data vendors\\nprovide them with access to mental health and wellness re-\\nsources and annotators have the right to opt out at any point.\\n7.4 Data Privacy and Security\\nTrustworthy handling of production data necessitates trans-\\nparency with users and effective security measures. We ob-\\ntain consent from all customers whose data is used to train\\nour moderation models. Customers who wish to opt their\\ndata out of training may do so. No production data is in-\\ncluded in the dataset we are releasing. Our data labeling and\\nactive learning pipelines feature security controls that are de-\\nsigned and tested to protect the conﬁdentiality and integrity\\nof production data. The model we deploy can not be used to\\ngenerate text, only to compute safety scores, so we consider\\nthe risk of training data leakage to be extremely low.\\n7.5 Summary of Broader Impacts Discussion\\nContent moderation classiﬁers are one key tool that em-\\npowers developers of language models at every stage of the\\nmodel development and deployment process- from working\\nwith large-scale datasets, to testing out models, to deploying\\nthe models to many users. However, as we have observed\\nabove, there are a range of normative and subjective deci-\\nsions made throughout the development process of build-\\ning these classiﬁers from designing taxonomies to labeling\\ndata. Given the nature of these tools, these decisions are\\nsometimes distilled down bluntly and do not enable captur-\\ning the nuances that the moderation decision may warrant.\\nThis loss of nuance may disproportionately impact mem-\\nbers of socially marginalized populations by muting their\\nopinions via unweighted majority annotations. This impact\\nis doubly grievous if moderation decisions about members\\nof marginalized populations are made about them by a sys-\\ntem that excludes their input. This highlights some inherent\\nlimitations of classiﬁers, using automated tools for content\\nmoderation, and point to the importance of their robust test-\\ning to ensure suitability for each speciﬁc use that they may\\nbe deployed in.\\n8 Conclusion\\nBuilding high-quality undesired content detection systems\\nin the real world is a challenge that requires the incorpora-\\ntion of multiple methods. A good content taxonomy is the\\nfoundation for problem scoping and data collection. A re-\\nliable data pipeline is needed to guarantee high data qual-\\nity and to handle distribution shift. We show that in cases\\nwhere certain target content occurs rarely, an active learning\\nsampling strategy leads to much better model performance.\\nAdditionally, we argue that good operational aspects of the\\nlabeling pipeline are essential for ensuring high data quality.\\nAnd we show that model performance can further be im-\\nproved through the use of curated synthetic data and semi-\\nsupervised learning.\\nAs large generative language models become more and\\nmore prevalent, it becomes increasingly important to de-\\nvelop ways of controlling and guiding their outputs. Thegoal of this work has been to demonstrate one way of imple-\\nmenting such control by way of building content detection\\nmodels. We are looking forward to further reﬁnement of our\\napproach in the future, as well as progress in other methods\\nof controlling and aligning generative model outputs.\\n9 Acknowledgments\\nThis work would not have been possible without the con-\\ntributions of data workers. We greatly appreciate their work\\nhandling sensitive content and helping us build better auto-\\nmated systems to make content moderation work less de-\\nmanding of human labor.\\nWe also thank Miles Brundage, Raf Jakubanis, Gretchen\\nKrueger, Derek Chen, Summer Yue, Karl Cobbe, Pranav\\nShyam, Jason Kwon and Matt Knight for feedback on this\\nwork.\\nReferences\\nAluru, S. S.; Mathew, B.; Saha, P.; and Mukherjee, A. 2020.\\nDeep learning models for multilingual hate speech detec-\\ntion. arXiv preprint arXiv:2004.06465 .\\nAlzantot, M.; Sharma, Y .; Elgohary, A.; Ho, B.-J.; Srivas-\\ntava, M.; and Chang, K.-W. 2018. Generating Natural Lan-\\nguage Adversarial Examples. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Language\\nProcessing , 2890–2896. Brussels, Belgium: Association for\\nComputational Linguistics.\\nAnaby-Tavor, A.; Carmeli, B.; Goldbraich, E.; Kantor, A.;\\nKour, G.; Shlomov, S.; Tepper, N.; and Zwerdling, N. 2020.\\nDo not have enough data? Deep learning to the rescue! In\\nProceedings of the AAAI Conference on Artiﬁcial Intelli-\\ngence , volume 34, 7383–7390.\\nArjovsky, M.; Chintala, S.; and Bottou, L. 2017. Wasser-\\nstein Generative Adversarial Networks. In Precup, D.; and\\nTeh, Y . W., eds., Proceedings of the 34th International Con-\\nference on Machine Learning , volume 70 of Proceedings of\\nMachine Learning Research , 214–223. PMLR.\\nBarbieri, F.; Camacho-Collados, J.; Espinosa Anke, L.; and\\nNeves, L. 2020. TweetEval: Uniﬁed Benchmark and Com-\\nparative Evaluation for Tweet Classiﬁcation. In Findings\\nof the Association for Computational Linguistics: EMNLP\\n2020 , 1644–1650. Association for Computational Linguis-\\ntics.\\nBarrientos, G. M.; Alaiz-Rodr ´ıguez, R.; Gonz ´alez-Castro,\\nV .; and Parnell, A. C. 2020. Machine learning techniques\\nfor the detection of inappropriate erotic content in text. In-\\nternational Journal of Computational Intelligence Systems ,\\n13(1): 591–603.\\nBen-David, S.; Blitzer, J.; Crammer, K.; Kulesza, A.;\\nPereira, F. C.; and Vaughan, J. W. 2009. A theory of learning\\nfrom different domains. Machine Learning , 79: 151–175.\\nBen-David, S.; Blitzer, J.; Crammer, K.; and Pereira, F. C.\\n2006. Analysis of Representations for Domain Adaptation.\\nInNIPS .\\nBlitzer, J.; McDonald, R. T.; and Pereira, F. C. 2006. Do-\\nmain Adaptation with Structural Correspondence Learning.\\nInEMNLP . Borkan, D.; Dixon, L.; Sorensen, J.; Thain, N.; and Vasser-\\nman, L. 2019. Nuanced Metrics for Measuring Unintended\\nBias with Real Data for Text Classiﬁcation. In Compan-\\nion Proceedings of The 2019 World Wide Web Conference ,\\nWWW ’19, 491–500. New York, NY , USA: Association for\\nComputing Machinery. ISBN 9781450366755.\\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\\nA.; et al. 2020. Language models are few-shot learners. Ad-\\nvances in neural information processing systems , 33: 1877–\\n1901.\\nCohen, A. D.; Roberts, A.; Molina, A.; Butryna, A.; Jin, A.;\\nKulshreshtha, A.; Hutchinson, B.; Zevenbergen, B.; Aguera-\\nArcas, B. H.; ching Chang, C.; Cui, C.; Du, C.; Adiwar-\\ndana, D. D. F.; Chen, D.; Lepikhin, D. D.; Chi, E. H.;\\nHoffman-John, E.; Cheng, H.-T.; Lee, H.; Krivokon, I.; Qin,\\nJ.; Hall, J.; Fenton, J.; Soraker, J.; Meier-Hellstern, K.; Ol-\\nson, K.; Aroyo, L. M.; Bosma, M. P.; Pickett, M. J.; Mene-\\ngali, M. A.; Croak, M.; D ´ıaz, M.; Lamm, M.; Krikun, M.;\\nMorris, M. R.; Shazeer, N.; Le, Q. V .; Bernstein, R.; Ra-\\njakumar, R.; Kurzweil, R.; Thoppilan, R.; Zheng, S.; Bos,\\nT.; Duke, T.; Doshi, T.; Prabhakaran, V .; Rusch, W.; Li, Y .;\\nHuang, Y .; Zhou, Y .; Xu, Y .; and Chen, Z. 2022. LaMDA:\\nLanguage Models for Dialog Applications. In arXiv .\\nCulotta, A.; and McCallum, A. 2005. Reducing labeling ef-\\nfort for structured prediction tasks. In AAAI , volume 5, 746–\\n751.\\nDagan, I.; and Engelson, S. P. 1995. Committee-based sam-\\npling for training probabilistic classiﬁers. In Machine Learn-\\ning Proceedings 1995 , 150–157. Elsevier.\\nDavani, A. M.; D ´ıaz, M.; and Prabhakaran, V . 2021. Dealing\\nwith Disagreements: Looking Beyond the Majority V ote in\\nSubjective Annotations. CoRR , abs/2110.05719.\\nDavidson, T.; Bhattacharya, D.; and Weber, I. 2019. Racial\\nBias in Hate Speech and Abusive Language Detection\\nDatasets. In Proceedings of the Third Workshop on Abu-\\nsive Language Online , 25–35. Florence, Italy: Association\\nfor Computational Linguistics.\\nDavidson, T.; Warmsley, D.; Macy, M.; and Weber, I. 2017.\\nAutomated hate speech detection and the problem of offen-\\nsive language. In Proceedings of the international AAAI\\nconference on web and social media , volume 11, 512–515.\\nde Gibert, O.; Perez, N.; Garc ´ıa-Pablos, A.; and Cuadros, M.\\n2018. Hate Speech Dataset from a White Supremacy Forum.\\nInProceedings of the 2nd Workshop on Abusive Language\\nOnline (ALW2) , 11–20. Brussels, Belgium: Association for\\nComputational Linguistics.\\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.\\nBERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding. In Burstein, J.; Doran, C.; and\\nSolorio, T., eds., Proceedings of the 2019 Conference of the\\nNorth American Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies, NAACL-\\nHLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Vol-\\nume 1 (Long and Short Papers) , 4171–4186. Association for\\nComputational Linguistics.Dinan, E.; Humeau, S.; Chintagunta, B.; and Weston, J.\\n2019. Build it break it ﬁx it for dialogue safety: Ro-\\nbustness from adversarial human attack. arXiv preprint\\narXiv:1908.06083 .\\nDwork, C.; Hardt, M.; Pitassi, T.; Reingold, O.; and Zemel,\\nR. 2012. Fairness through Awareness. In Proceedings of\\nthe 3rd Innovations in Theoretical Computer Science Con-\\nference , ITCS ’12, 214–226. New York, NY , USA: Associa-\\ntion for Computing Machinery. ISBN 9781450311151.\\nFeng, S.; Wallace, E.; Grissom II, A.; Iyyer, M.; Rodriguez,\\nP.; and Boyd-Graber, J. 2018. Pathologies of Neural Mod-\\nels Make Interpretations Difﬁcult. In Proceedings of the\\n2018 Conference on Empirical Methods in Natural Lan-\\nguage Processing , 3719–3728. Brussels, Belgium: Associ-\\nation for Computational Linguistics.\\nGal, Y .; Islam, R.; and Ghahramani, Z. 2017. Deep bayesian\\nactive learning with image data. In International Conference\\non Machine Learning , 1183–1192. PMLR.\\nGanin, Y .; and Lempitsky, V . S. 2015. Unsupervised Domain\\nAdaptation by Backpropagation. ArXiv , abs/1409.7495.\\nGanin, Y .; Ustinova, E.; Ajakan, H.; Germain, P.; Larochelle,\\nH.; Laviolette, F.; Marchand, M.; and Lempitsky, V . 2016.\\nDomain-Adversarial Training of Neural Networks. J. Mach.\\nLearn. Res. , 17(1): 2096–2030.\\nGao, T.; Yao, X.; and Chen, D. 2021. SimCSE: Simple Con-\\ntrastive Learning of Sentence Embeddings. In Empirical\\nMethods in Natural Language Processing (EMNLP) .\\nGarg, S.; Perot, V .; Limtiaco, N.; Taly, A.; Chi, E. H.; and\\nBeutel, A. 2019. Counterfactual Fairness in Text Classi-\\nﬁcation through Robustness. In Proceedings of the 2019\\nAAAI/ACM Conference on AI, Ethics, and Society , AIES\\n’19, 219–226. New York, NY , USA: Association for Com-\\nputing Machinery. ISBN 9781450363242.\\nGehman, S.; Gururangan, S.; Sap, M.; Choi, Y .; and\\nSmith, N. A. 2020. Realtoxicityprompts: Evaluating neu-\\nral toxic degeneration in language models. arXiv preprint\\narXiv:2009.11462 .\\nGoodfellow, I. J.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;\\nWarde-Farley, D.; Ozair, S.; Courville, A. C.; and Bengio,\\nY . 2014. Generative Adversarial Nets. In NIPS .\\nGoodfellow, I. J.; Shlens, J.; and Szegedy, C. 2015. Ex-\\nplaining and Harnessing Adversarial Examples. CoRR ,\\nabs/1412.6572.\\nGuo, H.; Mao, Y .; and Zhang, R. 2019. Augmenting data\\nwith mixup for sentence classiﬁcation: An empirical study.\\narXiv preprint arXiv:1905.08941 .\\nHartvigsen, T.; Gabriel, S.; Palangi, H.; Sap, M.; Ray, D.;\\nand Kamar, E. 2022. Toxigen: A large-scale machine-\\ngenerated dataset for adversarial and implicit hate speech\\ndetection. arXiv preprint arXiv:2203.09509 .\\nHoi, S. C.; Jin, R.; Zhu, J.; and Lyu, M. R. 2006. Batch mode\\nactive learning and its application to medical image classiﬁ-\\ncation. In Proceedings of the 23rd international conference\\non Machine learning , 417–424.\\nJigsaw. ???? Perspective API. https://www.perspectiveapi.\\ncom/. Accessed: 2022-06-15. Jigsaw. 2018. Toxic Comment Classiﬁcation Chal-\\nlenge. https://www.kaggle.com/competitions/jigsaw-toxic-\\ncomment-classiﬁcation-challenge/overview. Accessed:\\n2022-06-15.\\nKiela, D.; Bartolo, M.; Nie, Y .; Kaushik, D.; Geiger, A.; Wu,\\nZ.; Vidgen, B.; Prasad, G.; Singh, A.; Ringshia, P.; Ma, Z.;\\nThrush, T.; Riedel, S.; Waseem, Z.; Stenetorp, P.; Jia, R.;\\nBansal, M.; Potts, C.; and Williams, A. 2021. Dynabench:\\nRethinking Benchmarking in NLP. In Proceedings of the\\n2021 Conference of the North American Chapter of the As-\\nsociation for Computational Linguistics: Human Language\\nTechnologies , 4110–4124. Online: Association for Compu-\\ntational Linguistics.\\nKobayashi, S. 2018. Contextual Augmentation: Data Aug-\\nmentation by Words with Paradigmatic Relations. In Pro-\\nceedings of the 2018 Conference of the North American\\nChapter of the Association for Computational Linguistics:\\nHuman Language Technologies, Volume 2 (Short Papers) ,\\n452–457. New Orleans, Louisiana: Association for Compu-\\ntational Linguistics.\\nKumar, V .; Choudhary, A.; and Cho, E. 2020. Data augmen-\\ntation using pre-trained transformer models. arXiv preprint\\narXiv:2003.02245 .\\nKusner, M. J.; Loftus, J.; Russell, C.; and Silva, R. 2017.\\nCounterfactual Fairness. In Guyon, I.; Luxburg, U. V .; Ben-\\ngio, S.; Wallach, H.; Fergus, R.; Vishwanathan, S.; and Gar-\\nnett, R., eds., Advances in Neural Information Processing\\nSystems 30 , 4066–4076. Curran Associates, Inc.\\nKwok, I.; and Wang, Y . 2013. Locate the Hate: De-\\ntecting Tweets against Blacks. In Proceedings of the\\nTwenty-Seventh AAAI Conference on Artiﬁcial Intelligence ,\\nAAAI’13, 1621–1622. AAAI Press.\\nLees, A.; Tran, V . Q.; Tay, Y .; Sorensen, J.; Gupta, J.; Met-\\nzler, D.; and Vasserman, L. 2022. A new generation of\\nperspective api: Efﬁcient multilingual character-level trans-\\nformers. arXiv preprint arXiv:2202.11176 .\\nLewis, D. D.; and Catlett, J. 1994. Heterogeneous uncer-\\ntainty sampling for supervised learning. In Machine learn-\\ning proceedings 1994 , 148–156. Elsevier.\\nLewis, D. D.; and Gale, W. A. 1994. A sequential algorithm\\nfor training text classiﬁers. In SIGIR’94 , 3–12. Springer.\\nLuo, T.; Kramer, K.; Goldgof, D. B.; Hall, L. O.; Samson,\\nS.; Remsen, A.; Hopkins, T.; and Cohn, D. 2005. Active\\nlearning to recognize multiple types of plankton. Journal of\\nMachine Learning Research , 6(4).\\nMansour, Y .; Mohri, M.; and Rostamizadeh, A. 2008. Do-\\nmain Adaptation with Multiple Sources. In NIPS .\\nMcCallum, A.; and Nigam, K. 1998. Employing EM\\nand Pool-Based Active Learning for Text Classiﬁcation.\\nInProceedings of the Fifteenth International Conference\\non Machine Learning , ICML ’98, 350–358. San Fran-\\ncisco, CA, USA: Morgan Kaufmann Publishers Inc. ISBN\\n1558605568.\\nMishkin, P.; Ahmad, L.; Brundage, M.; Krueger, G.; and\\nSastry, G. 2022. DALL·E 2 Preview - Risks and Limitations.Mollas, I.; Chrysopoulou, Z.; Karlos, S.; and Tsoumakas,\\nG. 2020. ETHOS: an online hate speech detection dataset.\\narXiv preprint arXiv:2006.08328 .\\nNguyen, H. T.; and Smeulders, A. 2004. Active learning\\nusing pre-clustering. In Proceedings of the twenty-ﬁrst in-\\nternational conference on Machine learning .\\nNobata, C.; Tetreault, J.; Thomas, A.; Mehdad, Y .; and\\nChang, Y . 2016. Abusive Language Detection in Online\\nUser Content. In Proceedings of the 25th International Con-\\nference on World Wide Web , WWW ’16, 145–153. Republic\\nand Canton of Geneva, CHE: International World Wide Web\\nConferences Steering Committee. ISBN 9781450341431.\\nOvesdotter Alm, C. 2011. Subjective Natural Language\\nProblems: Motivations, Applications, Characterizations, and\\nImplications. In Proceedings of the 49th Annual Meeting of\\nthe Association for Computational Linguistics: Human Lan-\\nguage Technologies , 107–112. Portland, Oregon, USA: As-\\nsociation for Computational Linguistics.\\nPAI. 2021. https://partnershiponai.org/paper/responsible-\\nsourcing-considerations/.\\nPatton, D.; Blandfort, P.; Frey, W.; Gaskell, M.; and Kara-\\nman, S. 2018. Annotating Twitter Data from Vulnera-\\nble Populations: Evaluating Disagreement Between Domain\\nExperts and Graduate Student Annotators.\\nPavlopoulos, J.; Sorensen, J.; Dixon, L.; Thain, N.; and An-\\ndroutsopoulos, I. 2020. Toxicity detection: Does context re-\\nally matter? arXiv preprint arXiv:2006.00998 .\\nPerez, E.; Huang, S.; Song, F.; Cai, T.; Ring, R.; Aslanides,\\nJ.; Glaese, A.; McAleese, N.; and Irving, G. 2022. Red team-\\ning language models with language models. arXiv preprint\\narXiv:2202.03286 .\\nPrabhakaran, V .; Mostafazadeh Davani, A.; and Diaz, M.\\n2021. On Releasing Annotator-Level Labels and Informa-\\ntion in Datasets. In Proceedings of The Joint 15th Linguis-\\ntic Annotation Workshop (LAW) and 3rd Designing Meaning\\nRepresentations (DMR) Workshop . Punta Cana, Dominican\\nRepublic: Association for Computational Linguistics.\\nRamponi, A.; and Plank, B. 2020. Neural Unsuper-\\nvised Domain Adaptation in NLP—A Survey. ArXiv ,\\nabs/2006.00632.\\nReddit. 2022. Building Better Moderator Tools. Accessed:\\n2022-08-04.\\nRibeiro, M. T.; Wu, T.; Guestrin, C.; and Singh, S. 2020.\\nBeyond Accuracy: Behavioral Testing of NLP Models with\\nCheckList. In Proceedings of the 58th Annual Meeting of\\nthe Association for Computational Linguistics , 4902–4912.\\nOnline: Association for Computational Linguistics.\\nRosenthal, S.; Atanasova, P.; Karadzhov, G.; Zampieri,\\nM.; and Nakov, P. 2020. A Large-Scale Semi-Supervised\\nDataset for Offensive Language Identiﬁcation. arXiv\\npreprint arXiv:2004.14454 .\\nR¨ottger, P.; Vidgen, B.; Nguyen, D.; Waseem, Z.; Margetts,\\nH.; and Pierrehumbert, J. 2021. HateCheck: Functional\\nTests for Hate Speech Detection Models. In Proceedings\\nof the 59th Annual Meeting of the Association for Compu-\\ntational Linguistics and the 11th International Joint Con-\\nference on Natural Language Processing (Volume 1: Long Papers) , 41–58. Online: Association for Computational Lin-\\nguistics.\\nSap, M.; Card, D.; Gabriel, S.; Choi, Y .; and Smith, N. A.\\n2019. The Risk of Racial Bias in Hate Speech Detection. In\\nProceedings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics , 1668–1678. Florence, Italy:\\nAssociation for Computational Linguistics.\\nScheffer, T.; Decomain, C.; and Wrobel, S. 2001. Active\\nhidden markov models for information extraction. In Inter-\\nnational Symposium on Intelligent Data Analysis , 309–318.\\nSpringer.\\nSchick, T.; and Sch ¨utze, H. 2021. Generating datasets\\nwith pretrained language models. arXiv preprint\\narXiv:2104.07540 .\\nSchmidt, S.; Rao, Q.; Tatsch, J.; and Knoll, A. 2020. Ad-\\nvanced active learning strategies for object detection. In\\n2020 IEEE Intelligent Vehicles Symposium (IV) , 871–876.\\nIEEE.\\nSchohn, G.; and Cohn, D. 2000. Less is more: Active learn-\\ning with support vector machines. In ICML .\\nSettles, B.; and Craven, M. 2008. An analysis of active\\nlearning strategies for sequence labeling tasks. In proceed-\\nings of the 2008 conference on empirical methods in natural\\nlanguage processing , 1070–1079.\\nSeung, H. S.; Opper, M.; and Sompolinsky, H. 1992. Query\\nby committee. In Proceedings of the ﬁfth annual workshop\\non Computational learning theory , 287–294.\\nShah, D.; Lei, T.; Moschitti, A.; Romeo, S.; and Nakov, P.\\n2018. Adversarial Domain Adaptation for Duplicate Ques-\\ntion Detection. In Proceedings of the 2018 Conference on\\nEmpirical Methods in Natural Language Processing , 1056–\\n1063. Brussels, Belgium: Association for Computational\\nLinguistics.\\nShen, D.; Zheng, M.; Shen, Y .; Qu, Y .; and Chen, W. 2020.\\nA Simple but Tough-to-Beat Data Augmentation Approach\\nfor Natural Language Understanding and Generation. arXiv\\npreprint arXiv:2009.13818 .\\nShen, J.; Qu, Y .; Zhang, W.; and Yu, Y . 2018. Wasser-\\nstein Distance Guided Representation Learning for Domain\\nAdaptation. In AAAI .\\nShen, X.; and Zhai, C. 2005. Active feedback in ad hoc\\ninformation retrieval. In Proceedings of the 28th annual in-\\nternational ACM SIGIR conference on Research and devel-\\nopment in information retrieval , 59–66.\\nSiddhant, A.; and Lipton, Z. C. 2018. Deep Bayesian Ac-\\ntive Learning for Natural Language Processing: Results of a\\nLarge-Scale Empirical Study. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Language\\nProcessing , 2904–2909. Brussels, Belgium: Association for\\nComputational Linguistics.\\nSzegedy, C.; Zaremba, W.; Sutskever, I.; Bruna, J.; Erhan,\\nD.; Goodfellow, I. J.; and Fergus, R. 2013. Intriguing prop-\\nerties of neural networks. CoRR , abs/1312.6199.\\nTzeng, E.; Hoffman, J.; Saenko, K.; and Darrell, T. 2017.\\nAdversarial Discriminative Domain Adaptation. 2017 IEEE\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , 2962–2971.Vidgen, B.; and Derczynski, L. 2020. Directions in abu-\\nsive language training data, a systematic review: Garbage\\nin, garbage out. Plos one , 15(12): e0243300.\\nVidgen, B.; Harris, A.; Nguyen, D.; Tromble, R.; Hale, S.;\\nand Margetts, H. 2019. Challenges and frontiers in abusive\\ncontent detection. In Proceedings of the Third Workshop on\\nAbusive Language Online , 80–93. Florence, Italy: Associa-\\ntion for Computational Linguistics.\\nVidgen, B.; Thrush, T.; Waseem, Z.; and Kiela, D.\\n2020. Learning from the worst: Dynamically generated\\ndatasets to improve online hate detection. arXiv preprint\\narXiv:2012.15761 .\\nWang, C.; and Banko, M. 2021. Practical Transformer-based\\nMultilingual Text Classiﬁcation. In Proceedings of the 2021\\nConference of the North American Chapter of the Associa-\\ntion for Computational Linguistics: Human Language Tech-\\nnologies: Industry Papers , 121–129. Online: Association for\\nComputational Linguistics.\\nWang, S.; Liu, Y .; Xu, Y .; Zhu, C.; and Zeng, M. 2021a.\\nWant To Reduce Labeling Cost? GPT-3 Can Help. In\\nFindings of the Association for Computational Linguistics:\\nEMNLP 2021 , 4195–4205. Punta Cana, Dominican Repub-\\nlic: Association for Computational Linguistics.\\nWang, Z.; Yu, A. W.; Firat, O.; and Cao, Y . 2021b.\\nTowards zero-label language learning. arXiv preprint\\narXiv:2109.09193 .\\nWaseem, Z. 2016. Are You a Racist or Am I Seeing Things?\\nAnnotator Inﬂuence on Hate Speech Detection on Twitter.\\nInProceedings of the First Workshop on NLP and Computa-\\ntional Social Science , 138–142. Austin, Texas: Association\\nfor Computational Linguistics.\\nWei, J.; and Zou, K. 2019. EDA: Easy Data Augmentation\\nTechniques for Boosting Performance on Text Classiﬁcation\\nTasks. In Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the 9th Inter-\\nnational Joint Conference on Natural Language Processing\\n(EMNLP-IJCNLP) , 6383–6389. Hong Kong, China: Asso-\\nciation for Computational Linguistics.\\nWeidinger, L.; Mellor, J.; Rauh, M.; Grifﬁn, C.; Uesato, J.;\\nHuang, P.-S.; Cheng, M.; Glaese, M.; Balle, B.; Kasirzadeh,\\nA.; et al. 2021. Ethical and social risks of harm from lan-\\nguage models. arXiv preprint arXiv:2112.04359 .\\nWeiss, K. R.; Khoshgoftaar, T. M.; and Wang, D. 2016. A\\nsurvey of transfer learning. Journal of Big Data , 3: 1–40.\\nWelbl, J.; Glaese, A.; Uesato, J.; Dathathri, S.; Mellor, J.;\\nHendricks, L. A.; Anderson, K.; Kohli, P.; Coppin, B.; and\\nHuang, P.-S. 2021. Challenges in detoxifying language mod-\\nels.arXiv preprint arXiv:2109.07445 .\\nXu, Z.; Akella, R.; and Zhang, Y . 2007. Incorporating di-\\nversity and density in active learning for relevance feedback.\\nInEuropean Conference on Information Retrieval , 246–257.\\nSpringer.\\nYin, W.; and Zubiaga, A. 2021. Towards generalisable hate\\nspeech detection: a review on obstacles and solutions. PeerJ\\nComputer Science , 7: e598. Yoo, K. M.; Park, D.; Kang, J.; Lee, S.-W.; and Park, W.\\n2021. GPT3Mix: Leveraging large-scale language models\\nfor text augmentation. arXiv preprint arXiv:2104.08826 .\\nYouTube. 2019. The Four Rs of Responsibility, Part 1: Re-\\nmoving Harmful Content. Accessed: 2022-08-04.\\nZampieri, M.; Malmasi, S.; Nakov, P.; Rosenthal, S.; Farra,\\nN.; and Kumar, R. 2019. Predicting the Type and Target of\\nOffensive Posts in Social Media. In Proceedings of the 2019\\nConference of the North American Chapter of the Associa-\\ntion for Computational Linguistics: Human Language Tech-\\nnologies, Volume 1 (Long and Short Papers) , 1415–1420.\\nAssociation for Computational Linguistics.\\nZeng, X.; Garg, S.; Chatterjee, R.; Nallasamy, U.; and\\nPaulik, M. 2019. Empirical evaluation of active learning\\ntechniques for neural MT. In Proceedings of the 2nd Work-\\nshop on Deep Learning Approaches for Low-Resource NLP\\n(DeepLo 2019) , 84–93.\\nZhang, C.; Zhao, J.; Zhang, H.; Chang, K.-W.; and Hsieh,\\nC.-J. 2021. Double Perturbation: On the Robustness of Ro-\\nbustness and Counterfactual Bias Evaluation. In Proceed-\\nings of the 2021 Conference of the North American Chap-\\nter of the Association for Computational Linguistics: Hu-\\nman Language Technologies , 3899–3916. Online: Associa-\\ntion for Computational Linguistics.\\nZhao, J.; Wang, T.; Yatskar, M.; Ordonez, V .; and Chang, K.-\\nW. 2017. Men Also Like Shopping: Reducing Gender Bias\\nAmpliﬁcation using Corpus-level Constraints. In Proceed-\\nings of the 2017 Conference on Empirical Methods in Nat-\\nural Language Processing , 2979–2989. Copenhagen, Den-\\nmark: Association for Computational Linguistics.\\nZiegler, D. M.; Nix, S.; Chan, L.; Bauman, T.; Schmidt-\\nNielsen, P.; Lin, T.; Scherlis, A.; Nabeshima, N.; Weinstein-\\nRaun, B.; de Haas, D.; et al. 2022. Adversarial Training for\\nHigh-Stakes Reliability. arXiv preprint arXiv:2205.01663 .\\nA Experiment Details\\nTable 6 presents how we map model taxonomies into la-\\nbels of different evaluation datasets. Some of the mappings\\nare only approximation. For example, Perspective deﬁnes\\n”threat” as ”Describes an intention to inﬂict pain, injury,\\nor violence against an individual or group.”, not includ-\\ning graphic violence, so not a perfect match for our ”vio-\\nlence” category. Either or our taxonomy has a good match\\nfor ”toxic”, ”severe toxic”, or ”offensive.\\nOur Evaluation Set. We are aware that about 4% of our\\nevaluation samples are in non-English. Perspective API call\\ntakes the language as an input parameter, but multilingual is\\nnot supported for several attributes. We instead use ”en” for\\nall the calls.\\nJigsaw. Jigsaw dataset is pretty large and we include about\\nhalf of it into our training set to resolve the cold-start prob-\\nlem. Among the rest half, we sampled 5000 examples for\\nevaluation.\\nTweetEval. We take the TweetEval (Barbieri et al. 2020)\\ntest datasets7on ”hate” and ”offensive”. There are in total\\n7https://github.com/cardiffnlp/tweeteval/tree/main/datasets2970 samples in the hate task test set and 860 in the offensive\\none.\\nStormfront. We use the test dataset of de Gibert et al.\\n(2018)8, containing 478 samples.\\nReddit. We downsampled 5000 examples from the ”RS -\\n201501” snapshot of Reddit pushshift datasets9and assigned\\nnoisy binary label to each example on whether it contains\\nsexual content according to the subreddits as listed in Barri-\\nentos et al. (2020).\\n8https://github.com/Vicomtech/hate-speech-dataset\\n9https://ﬁles.pushshift.io/reddit/submissions/ Taxonomy Perspective Ours\\nOurs Sexual max(sexually explicit, profanity, ﬂirtation) sexual\\nHate identity attack hate\\nViolence threat violence\\nHarassment max(toxicity, severe toxicity, insult, threat) harassment\\nSexual/minors - sexual/minors\\nJigsaw Toxic toxicity harassment\\nObscene max(sexually explicit, profanity) sexual\\nThreat threat violence\\nInsult insult max(harassment, hate)\\nIdentity hate identity attack hate\\nTweetEval Hate identity attack hate\\nOffensive max(toxicity, severe toxicity, threat, insult,\\nidentity attack)harassment\\nStormfront Hate identity attack hate\\nReddit Sexual max(sexually explicit, profanity, ﬂirtation) sexual\\nTable 6: How taxonomies of different APIs get mapped into labels of various evaluation datasets.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A Holistic Approach to Undesired Content Detection in the Real World\\nWarning: some content may contain racism, sexuality, or other harmful language.\\nTodor Markov*Chong Zhang*Sandhini Agarwal Tyna Eloundou\\nTeddy Lee Steven Adler Angela Jiang Lilian Weng*\\nOpenAI\\nAbstract\\nWe present a holistic approach to building a robust and useful\\nnatural language classiﬁcation system for real-world content\\nmoderation. The success of such a system relies on a chain of\\ncarefully designed and executed steps, including the design\\nof content taxonomies and labeling instructions, data qual-\\nity control, an active learning pipeline to capture rare events,\\nand a variety of methods to make the model robust and to\\navoid overﬁtting. Our moderation system is trained to detect\\na broad set of categories of undesired content, including sex-\\nual content, hateful content, violence, self-harm, and harass-\\nment. This approach generalizes to a wide range of different\\ncontent taxonomies and can be used to create high-quality\\ncontent classiﬁers that outperform off-the-shelf models.\\n1 Introduction\\nRecent advances in deep learning have accelerated the adop-\\ntion of language models for socioeconomically valuable\\ntasks in the real world (Devlin et al. 2019; Brown et al.\\n2020; Cohen et al. 2022). Both the systems’ builders and its\\nusers may beneﬁt from a responsible deployment approach\\nthat includes moderating the models’ outputs: First, model\\nproviders may want assurances that the models will not pro-\\nduce content that is disallowed by their policies. Second,\\ncustomers of these models sometimes require control over\\ncontent to mitigate the impact of sensitive use cases or to\\nreduce brand risk. A principled, robust, and efﬁcient moder-\\nation solution can track and measure the model inputs and\\noutputs to ensure safety standards. It can also provide ﬁne-\\ngrained control to enable use cases with sensitive needs,\\nsuch as educational applications. We believe that a strong\\nundesired content classiﬁer lays the foundation for build-\\ning safer AI systems in the wild, as it enables the capacity\\nof moderating, evaluating, and guiding the models towards\\nsafer behavior.\\nExisting work on content detection either focuses mainly\\non a limited set of categories, including toxicity (Pavlopou-\\nlos et al. 2020; Gehman et al. 2020), hate speech (Kwok and\\nWang 2013; Davidson et al. 2017), and abusive content (No-\\nbata et al. 2016; Vidgen et al. 2019); or is tailored towards a\\ntargeted use case, such as Perspective API (Jigsaw) on online\\nCopyright © 2023, Association for the Advancement of Artiﬁcial\\nIntelligence (www.aaai.org). All rights reserved.\\n*These authors contributed equally to this work\\nFigure 1: Overview of the model training framework.\\ntoxic comment moderation. There is increasing attention to\\nunderstanding the risk areas of large language models via\\na more rigorous taxonomy (Weidinger et al. 2021), but the\\namount of work is still limited, especially when it comes\\nto deploying language models for real-', 'world applications.\\nHere we build a more comprehensive system for detecting a\\nbroad set of categories of undesired content, including sex-\\nual content, hateful content, violence, self-harm, and harass-\\nment, as well as severe subcategories under each top-level\\ncategory. Large-scale content moderation systems and tool-\\ning exist on a number of platforms (YouTube 2019; Reddit\\n2022). We aim to provide a blueprint for creating such sys-\\ntems across a wide variety of use cases.\\nDetecting undesired content is difﬁcult due to several\\nchallenges. First, there is not a clearly and widely agreed-\\nupon categorization of undesired content. Designing a de-\\ntailed taxonomy for undesired content and operationalizing\\nit for labeling purposes require a lot of work. The categoriza-\\ntion framework usually needs to clarify a signiﬁcant number\\nof corner cases to achieve high inter-rater agreement dur-\\ning labeling. This is further complicated by the subjectivity\\nof some labeling decisions, due to the different social and\\ncultural backgrounds of human annotators. Second, a prac-arXiv:2208.03274v2  [cs.CL]  14 Feb 2023 tical moderation system needs to process real-world trafﬁc.\\nThus a model bootstrapped from public data or academic\\ndatasets would not work well because there exists a big\\ndata distribution shift and taxonomy misalignment. Third,\\nit is rare to encounter certain categories of undesired content\\nin real-world settings. For example, among sampled user\\nprompts we observed that only 0.04% of cases included self-\\nharm and 0.017% included hateful content involving threats.\\nHence, we need smart solutions to the cold start problem and\\neffective ways to discover undesired samples.\\nMultiple components contribute to the success of building\\nand deploying a practical, general moderation system into\\nthe real world. These include effectively establishing a chain\\nof carefully polished and curated conﬁgurations for data col-\\nlection, data labeling, model training and active learning.\\nBased on our experimentation, we ﬁnd the following con-\\nclusions to be especially noteworthy.\\n•Detailed instructions and quality control are needed to\\nensure data quality. Labeling instructions that lack suf-\\nﬁcient precision force annotators to rely on their sub-\\njective judgment, resulting in inconsistently labeled data\\nthat confuses the model. Regular calibration sessions are\\nnecessary to reﬁne these instructions and ensure annota-\\ntors are aligned with them. And a poorly chosen qual-\\nity metric can lead to data that hurts model performance.\\n(See §3.2)\\n•Active learning is a necessity. There is likely a large dis-\\ntribution shift between public data and the trafﬁc from\\none’s production system. Thus, it is critical to collect\\nnew training samples from the production trafﬁc. Ac-\\ntive learning can effectively expand the training dataset\\nto capture a signiﬁcantly (up to 22 \\x02) larger amount of\\nundesired samples when dealing with rare events. This\\ncan lead to a performance improvement in the underl', 'y-\\ning model of up to 10 \\x02for rare categories. (See §3.1 and\\n§4.3)\\n•Use public datasets with care. Publicly available data\\nmight not lead to high quality performance for the prob-\\nlem in hand due to differences in taxonomy and training\\ndata distribution, but can be used to construct a noisy cold\\nstart dataset at the early stage. However, adding academic\\ndata into the training set may hurt the model performance\\nat a later stage when there are enough properly labeled\\ndata samples. (See 3.5)\\n•Imbalanced training data can lead to incorrect gen-\\neralization. Deep learning models could easily overﬁt\\ncommon phrases or templates. For example, the model\\ncan over-generalize to anything formatted as \"X is\\nhateful\" if the data distribution is off-balance. We\\ntackle this challenge by programmatically identifying\\noverﬁtted phrases and by red-teaming via human tri-\\nals. We then alter the training distribution by incorpo-\\nrating model-generated or human-curated synthetic data\\nto patch the weakness. (See §3.5 and §3.3)\\n•Mistakes in data will happen and need to be managed.\\nEven with signiﬁcant amounts of effort on data qual-\\nity control, we still run into mislabeled examples. We\\nexplore different methods for identifying those cases,including cross-validation and hunting down common\\nphrases causing the model to overﬁt via token subtrac-\\ntion. (See §3.2 and §3.5)\\nWe aim to present a holistic approach to building a reli-\\nable and robust undesired content detection model for real-\\nworld applications. Our approach is meant to handle the sce-\\nnario in which the type of the content to be detected is rarely\\nobserved in the real world. We hope that the lessons we\\nlearned are useful to others working on similar problems.\\nWe release a dataset1containing text samples from publicly\\navailable data labeled according to our taxonomy. The model\\nis accessible via the OpenAI Moderation endpoint2.\\n2 Taxonomy\\nGiven that the meaning of a sentence can depend on con-\\ntext, designing a universal taxonomy for undesired content\\nis challenging and requires inherent trade-offs. Throughout\\nthis exercise, we aimed to make the dataset as broadly appli-\\ncable as possible to maximize its utility across research and\\nindustrial contexts, but recognize it still ought to be critically\\nevaluated prior to use in any context, The anatomy of what’s\\nundesired can vary based on factors like use cases, users, re-\\ngions and many others. Some sentences labeled as undesired\\naccording to our taxonomy (e.g. \"I will kill you\"\\nwould be labeled as containing violence) may not be con-\\nsidered undesired when appearing in another context such\\nas ﬁction.\\nAdditionally, we aimed to capture layers of subcategoriza-\\ntion and granularity. Much existing work lacks subcatego-\\nrizations to capture a spectrum of severity, which is critical,\\nsince statements such as \"kill all [IDENT GROUP]\"\\ncan have different repercussions compared to \"[IDENT -\\nGROUP] are dishonest\" . The existence of subcate-\\ngories can also largely reduce the d', 'isagreement rate and cor-\\nner cases during labeling, making it valuable for data quality\\ncontrol. In the following outline, we present our taxonomy\\nfor undesired content categorization, including ﬁve top-level\\ncategories and a spectrum of subcategories.\\nS: Sexual content Content that depicts explicit or implicit\\nsexual behavior. Subcategories that are considered unde-\\nsired: [ S3] sexual content involving minors, [ S2] content\\nthat depicts certain sexual activities which could be ille-\\ngal3if they happened in real life today, and [ S1] erotic\\nsexual content that does not depict illegal activities. Sub-\\ncategories that are not considered undesired: [ S0] Non-\\nerotic or contextualized sexual content, such as medical\\nor sex education material.\\nH: Hateful content Content that is threatening, insulting,\\nderogatory and otherwise abusive content targeting spe-\\nciﬁc chosen groups or members of the group because of\\ntheir group identities. Subcategories that are considered\\n1https://github.com/openai/moderation-api-release; sourced\\nfrom CommonCrawl and model-generated data\\n2https://beta.openai.com/docs/guides/moderation; Harassment\\ncategory is currently under further improvement and will be avail-\\nable in the future.\\n3This mapped most closely to what’s illegal in USA. undesired: [ H2] hateful content calling for violence or\\nthreatening, [ H1] derogatory stereotypes or support for\\nhateful statements. Subcategories that are not considered\\nundesired: [ H0.a ] neutral statement referring to group\\nidentity and [ H0.b ] contextualized hate speech, such as\\na quote of other individual’s statement in order to provide\\ncommentary.\\nV: Violence Content that depicts or shows support for phys-\\nical violence. Subcategories that are considered unde-\\nsired: [ V2] extremely graphic violence, [ V1] threats or\\nsupport for violence. Subcategories that are not consid-\\nered undesired: [ V0] neutral depictions of contextualized\\nviolence.\\nSH: Self-harm Content that contains a graphic description\\nor shows support for self-harm.\\nHR: Harassment Content that may be used to torment or\\nannoy individuals in real life, or make harassment more\\nlikely to occur.\\nOur model learns to predict whether a given sample vio-\\nlates any of 8 chosen categories , including all the top cate-\\ngories ( S,H,V,SH,HR) and three most severe subcategories\\n(S3,H2, andV2).\\n3 Methods\\n3.1 Data Selection and Active Learning\\nTo ensure that our moderation system performs well in the\\ncontext of our production use cases, we need to incorporate\\nproduction data to our training set. We set up a three-stage\\nprocedure in an iterative fashion.\\nFirst, a large volume of our production data is selected\\nat random. Any potential personally identiﬁable information\\n(PII) is masked. The most recent moderation model is used\\nto score these samples and discover which ones may trigger\\nany chosen categories.\\nIn the second stage we run a simple active learning strat-\\negy to select a subset of most valuable samples to be labeled\\nout of the ra', 'ndom samples extracted in stage one. The ac-\\ntive learning strategy is composed of three parallel pipelines.\\nThe ﬁrst one relies on random sampling such that some frac-\\ntion of our data remain consistent with the underlying data\\ndistribution in production. The second one randomly selects\\nfrom samples with model score above a certain threshold\\nfor each category to identify likely undesired data points.\\nThe last pipeline adopts a set of uncertainty sampling strate-\\ngies (Lewis and Gale 1994; Lewis and Catlett 1994) to cap-\\nture samples that the model is most uncertain about, where\\nthe model score for that category is closest to 0.5.\\nDuring the ﬁnal stage, all the samples selected by differ-\\nent active learning strategies are aggregated and re-weighted\\nbased on statistics of certain metadata associated with it.\\nThe sampling weight is conﬁgured to be proportional to\\nthe square root of the sample count. This helps improve\\nthe diversity of selected samples with regard to the associ-\\nated metadata. We update the sub-strategy mixture over time\\nbased on changes in the data distribution and categories that\\nwe want to improve the most at different stages.3.2 Labeling and Quality Control\\nData label correctness is critical to good model performance.\\nGetting such data can be difﬁcult given that our categories\\nand the boundary lines between them are inherently sub-\\njective. However, certain interventions can signiﬁcantly im-\\nprove the quality of labeled data.\\nOne important intervention for improving data quality\\n- in terms of both consistent labels across different anno-\\ntators as well as between annotators and researchers - is\\nto make the labeling instructions as well-deﬁned andcon-\\ncrete as possible. To make the instructions well-deﬁned, we\\nsought to design detailed deﬁnitions and design categories\\nor subcategories to be as mutually exclusive as possible so\\nas to minimize ambiguity. To make the instructions concrete,\\nwe hosted regular calibration sessions to review ambiguous\\nedge cases and instances where external annotators and our\\ninternal auditors disagree. Based on feedback from those\\nsessions, we made the instructions more clear and concrete,\\nwith numerous examples and clearer deﬁnitions around bor-\\nderline cases. As rules are deﬁned clearly and concretely to\\nminimize subjective judgments, they can be executed more\\nconsistently by the annotators.\\nRegular, ongoing audits are necessary to ensure that la-\\nbeled data continues to be of sufﬁciently high quality. The\\nchoice of which samples to audit and what metrics to use\\nto measure data quality is crucial. We found that selecting\\nauditing targets at random cannot maximize the value out\\nof auditing due to the imbalanced distribution across cate-\\ngories. The annotator-auditor agreement rate (i.e. accuracy)\\nis suboptimal because undesired examples are rare events\\nto encounter and the accuracy can be arbitrarily high due\\nto the abundance of true negatives. Instead, in each cho-\\nsen category, we randomly select 1', '0 samples labeled as\\nundesired and 10 samples with model probability greater\\nthan 50%. The former help capture false positive cases and\\nthe latter provide an estimation on recall. Then we com-\\npute the F-1 score for the chosen samples based on the\\nannotator-assigned labels while using auditor-assigned la-\\nbels as ground truth. This procedure performs much better\\nin practice when certain categories of undesired data points\\nare rare. Separation of metrics per category makes it easy to\\nrecognize category-speciﬁc issues and to retrain annotators\\naccordingly.\\nEven with very clear labeling instructions and an effec-\\ntive audit procedure, mistakes in data are still unavoidable.\\nTo identify potentially mislabeled samples in our dataset,\\nwe periodically split our current training dataset into two\\nparts, train separate models on those datasets and use each\\nmodel to score another half of the dataset that model was\\nnot trained on. When the model prediction disagrees with\\nthe current ground-truth label, the sample in question gets\\nﬂagged. A random portion of ﬂagged samples is audited, and\\nif more than 30% are identiﬁed as mislabeled, all ﬂagged\\nsamples would get labeled again for the second time.\\n3.3 Synthetic Data\\nIn addition to the data collection discussed above, we also\\nuse synthetic data to improve model performance on rare Example prompt\\n>>The text is about self harm, more speciﬁcally: physical self-harm (cutting).\\n>>The narrator is third person (a friend).\\n>>The narrator is a young teenager (female).\\n>>The act of self harm happens: last week.\\n>>Type of text: online forum post.\\n>>The following word should come up in the text: camera.\\n>>The text should not contain the phrase ”self harm” or ”self-harm”.\\n>>The writing level is: average, some errors.\\n>>The text starts below:\\nTable 1: Example zero-shot prompt template for generating\\nsynthetic SHdata. The sections in green are ﬁlled with ran-\\ndom ingredients to encourage diversity.\\ncategories such as SHand to mitigate the counterfactual bias\\ntowards certain demographic attributes (Kusner et al. 2017;\\nGarg et al. 2019; Dwork et al. 2012). Generating synthetic\\ndata through large pre-trained language models has shown\\nto be an effective way for data augmentation (Anaby-Tavor\\net al. 2020; Kumar, Choudhary, and Cho 2020; Yoo et al.\\n2021) and it is particularly helpful when there is little to no\\ninitial data (“cold start”) or when there are not enough un-\\ndesired samples in the production trafﬁc.\\nZero-shot data for cold start. To kick start the active\\nlearning and labeling process, we need some initial data to\\nbuild the ﬁrst version of the model and train annotators.\\nHowever, it is difﬁcult to ﬁnd existing public datasets on\\ncertain categories such as SHandV2. We tackle the prob-\\nlem by generating a synthetic dataset with zero-shot prompts\\non GPT-3. The prompts are constructed from human-crafted\\ntemplates and we label the generated texts as the initial\\ndataset. Table 1 provides an example prompt for SH.\\nFew-shot dat', 'a for rare categories. Some sub-categories\\nhad minimal amounts of undesired data even after several\\niterations of active learning. To address this, we constructed\\nfew-shot prompts with existing undesired examples and sent\\nthe generated texts to be labeled. The generated texts are\\nmanually inspected to avoid bias ampliﬁcation (Zhao et al.\\n2017). We observed a nontrivial performance improvement\\nby incorporating the synthetic dataset.\\nCurated data to mitigate counterfactual bias. Similar\\nto other existing NLP models, our models also suffer from\\ncounterfactual bias towards certain demographic attributes\\nas bias commonly exists in the training data. For instance,\\n\"black women.\" was classiﬁed as hateful content with\\nhigh conﬁdence in earlier versions of the model. We mitigate\\nthe issue by curating a synthetic dataset with templates that\\ntend to lead to hateful predictions, e.g., \"[subject] is\\nselfish/foolish/narrow-minded.\" . The [sub-\\nject] could either be ﬁlled with real demographic at-\\ntributes (e.g., Latino ) or random object names (e.g.,\\n\"black blanket\" ), which forms hateful and safe sam-\\nples respectively. We observe that the curated dataset not\\nonly mitigates bias to some degree, but also helps improve\\nthe model performance. For instance, the average AUPRC\\non hateful content was improved from 0:417 to0:551 by\\nadding 69k curated synthetic examples. We believe this is\\nbecause the contrastive setup of subjects in synthetic exam-ple templates encourages the model to infer the correct fea-\\nture representations: negative descriptive words or individ-\\nual identity groups alone are not enough to be considered\\nhateful, and only when they appear together they might be\\nconsidered hateful. Despite the observed improvements, the\\nsynthetic dataset also has limitations and we will continue\\nimproving it in the future (§6).\\nLarge amount of noisy data does not help. To under-\\nstand whether it is helpful to include a large amount of noisy\\nsynthetic data, we also generated zero-shot and few-shot ex-\\namples twice the size of the existing labeled training dataset.\\nFor zero-shot examples, we set the label to positive or neg-\\native if the prompt asks the model to generate undesired or\\nsafe examples, respectively. For few-shot examples, we set\\nthe label to positive or negative if all of the few-shot exam-\\nples are undesired or safe, respectively. Contrary to previous\\nstudies (Wang et al. 2021b; Schick and Sch ¨utze 2021), we\\nfound mixing noisy synthetic data into training hurt model\\nperformance. It is worth noting that many existing stud-\\nies on synthetic data usage experimented in the no-to-low\\ndata regime, where only a handful of labels are available.\\nHowever, in our experiment, we have collected a large high-\\nquality dataset and we suspect that noise introduced by syn-\\nthetic data confuses the model and lowers the learning efﬁ-\\nciency.\\n3.4 Domain Adversarial Training\\nWe intended to make good use of existing public NLP\\ndatasets to improve the performance of our model', 's. How-\\never, we observed that models trained on public NLP\\ndatasets do not perform well on our production trafﬁc. This\\nis likely due to the distribution difference between domains.\\nFor instance, examples from our production trafﬁc are usu-\\nally much longer and contain few-shot prompts, whereas\\nexisting public NLP datasets are usually shorter and often\\ncrawled from Wikipedia, Twitter, etc. (Vidgen and Derczyn-\\nski 2020). To mitigate the problem, besides carefully tuning\\nthe mixture of public datasets and production data, we in\\naddition apply Wasserstein Distance Guided Domain Adver-\\nsarial Training (WDAT) to encourage the model to learn do-\\nmain invariant representations (Arjovsky, Chintala, and Bot-\\ntou 2017; Ganin et al. 2016).\\nWe follow Shen et al. (2018) and approximate the Wasser-\\nstein distance by maximizing the loss of a domain critic\\nhead. Letfz(x) :Rd!Rzbe the feature extractor that\\nmaps thed-dimensional input into a z-dimensional embed-\\nding,fc(h) :Rz!Rcbe a multiclass classiﬁcation head,\\nandfd(h) :Rz!Rbe the domain critic head that maps\\nthe embedding into real number. The domain critic loss is\\ndeﬁned as\\nLd(Ds;Dt) =jE\\nx2Dsfd(fz(x))\\x00E\\nx2Dtfd(fz(x))j:\\nCombined with the regular classiﬁcation loss Lc, our objec-\\ntive is to solve the following minimax problem:\\nmin\\n\\x12z;\\x12cfLc+\\x15max\\n\\x12dLdg; where\\x12z;\\x12c;\\x12dare the parameters of fz;fc;fd, respec-\\ntively. Our model uses a transformer encoder as the feature\\nextractorfz.\\nIn our implementation, we use the absolute value in Ld\\nsince the initial loss could be negative, and clip \\x12din a com-\\npact space [\\x000:01;0:01]to enforce the Lipchitz constraint.\\nWe empirically set the balancing coefﬁcient \\x15to 0.01. In ex-\\nperiments, WDAT achieves a more stable training compared\\nto the original classiﬁer-based approach (Arjovsky, Chintala,\\nand Bottou 2017), and yields better performance on our pro-\\nduction trafﬁc with and without labeled production data in\\nthe training set.\\n3.5 Model Probing\\nIt is widely known that ML models are vulnerable to ad-\\nversarial inputs and may make predictions based on seem-\\ningly irrelevant features (Szegedy et al. 2013; Goodfellow,\\nShlens, and Szegedy 2015; Alzantot et al. 2018; Zhang et al.\\n2021). For instance, a sentiment classiﬁcation model may\\nmake different predictions for \"a short and moving\\nfilm.\" and\"a short and moving movie.\" , in-\\ndicating that the model makes predictions based on film/-\\nmovie rather than descriptive words. To make sure that our\\nmodel performs as expected, we conduct tool-assisted model\\nprobing and human-in-the-loop red-teaming.\\nKey Tokens Probing. Our ﬁrst goal is to verify whether\\nthe model is making predictions based on correct features.\\nFor instance, we observed that our models may incorrectly\\npredict \"all immigrants.\" ashate , which is pos-\\nsibly because among 21 training examples containing the\\nphrase (from R ¨ottger et al. (2021)), 17 examples are unde-\\nsired and only 4 examples are safe. Table 2 provides a few\\nexamples, where the expected behavior is to also ', 'take k1ll\\nandD3ath into account. To identify such potentially over-\\nﬁtted key tokens, we apply input reduction (Feng et al. 2018)\\non the training dataset by removing tokens greedily until the\\nprediction drops below 0.8. On average, we were able to\\nreduce the number of characters per sample from 722.3 to\\n15.9. By cross-validating with third-party APIs and heuris-\\ntic keywords, we infer that about 97% of the identiﬁed key\\ntokens are indeed unsafe, indicating that the model is pick-\\ning up the correct feature most of the time. For the remaining\\n3% key tokens, we manually reviewed them and discovered\\nthe following problems:\\n1. The frequency of irrelevant symbols is high (e.g., the pe-\\nriod symbol contributes to the prediction score).\\n2. Some key tokens are long and close to the original exam-\\nple, hinting that the model memorizes training samples.\\n3. Our model is not strong at handling typos and adversarial\\ninputs (e.g., \"w h o r e s\" becomes \"e s\" ).\\nWe partially addressed the above issues, and plan to re-\\nsolve with better data augmentation in the future (§6).\\nHuman Red-teaming. As a ﬁnal layer of the veriﬁcation\\nprocess, we conduct internal red-teaming before releasing\\nnew models. The goal of the red-teaming is to uncover any\\nunexpected model weaknesses beyond the test dataset. Dur-\\ning the process, we discovered the following issues:Hate Example\\n1 I want to k1ll all immigrants.\\n1 Why is it that all immigrants are repulsive?\\n1 D3ath to all immigrants.\\n0 Threatening to kill all immigrants will get\\nyou thrown in jail.\\n:::\\nTable 2: Example training data from (R ¨ottger et al. 2021)\\ncontaining the phrase \"all immigrants.\" The model\\ntrained on it directly may make predictions based on incor-\\nrect features.\\n1. The model predicts high hate scores for examples con-\\ntaining the “#” token. This phenomenon is likely caused\\nby the fact that we have many hate training exam-\\nples from some academic datasets that contain only short\\ntweets.\\n2. The model predicts high hate and violence\\nscores for examples containing potential racial to-\\nkens such as black . It is expected to classify \"I\\nhate black people!\" ashate but not \"I hate\\nblack cats!\" ashate .\\nTo mitigate the above issues, we construct synthetic\\ndatasets from hand-curated templates and synthetic model\\ngenerations to patch the holes (§3.3), and adjust the train-\\ning dataset distribution to make sure we have the right\\nmix across multiple types of text sourced from academic\\ndatasets. The process can be iterative, helping us discover\\nnew issues and solutions in each round and naturally lead-\\ning to improved robustness and consistency in time when the\\nred-teaming process can be executed more regularly and at\\nscale.\\n4 Experiment Results\\n4.1 Model Architecture and Training\\nOur model is a lightweight transformer decoder model\\nwhere the ﬁnal output linear layer is replaced with 8 MLP\\nheads, each corresponding to one independent matrix of\\nshape [dmodel;256;1], wheredmodel is the transformer model\\nsize. We ﬁnd this he', 'ad architecture works better than a single\\ndeep MLP layer with one output vector of 8 dimensions at\\navoiding interference between categories and requires fewer\\nparameters to train.\\nThe model is initialized from a GPT model that is pre-\\ntrained on a large text corpus and then ﬁne-tuned with learn-\\ning rate 0.05, batch size 256, dropout rate 0.1 within MLP\\nheads and up to 3 epochs.\\n4.2 Model Performance\\nOur model is trained and tested on both production and pub-\\nlic data. We are not able to share the test dataset contain-\\ning production trafﬁc for privacy and legal reasons; hence,\\nwe report the model performance on a different test dataset4\\ncontaining only samples from public data, as well as several\\npublicly available datasets on undesired content detection.\\n4https://github.com/openai/moderation-api-release Perspective Ours\\nPublic S .8709* .9703\\nH .6914 .7968\\nV .5201 .7371\\nHR .3902* .6191\\nSH - .8070\\nS3 - .7638\\nH2 - .7268\\nV2 - .6061\\nJigsaw Identity-hate .6644 .6890\\nInsult .8814 .8548\\nObscene .9500 .8353*\\nThreat .7492 .6144*\\nToxic .9769 .9304*\\nTweetEval Hate .5961 .6473\\nOffensive .7919* .7024*\\nStormfront Hate .8754 .9053\\nReddit Sexual .8961* .9417*\\nTable 3: Comparison of our model with Perspective API on\\nAUPRC (Area under the Precision-Recall Curve) across a\\nset of test datasets. Numbers followed with ”*” are based on\\napproximated taxonomy match, so not an exact fair compar-\\nison.\\nTable 3 compares the performance of our model with\\nPerspective API5as a baseline on our test dataset,\\nTweetEval (Barbieri et al. 2020), Stormfront hate speech\\ndataset (de Gibert et al. 2018), a subset of Reddit com-\\nments with noisy labels on erotic content processed accord-\\ning to Barrientos et al. (2020) and a downsampled Jigsaw\\ntoxic comments test dataset (Jigsaw 2018). None of the\\ntraining portion of external evaluation benchmarks are incor-\\nporated into our training, except for half of Jigsaw’s training\\ndata that has no overlap with the Jigsaw test set in evaluation.\\nUnfortunately, due to the taxonomy mismatch, we cannot\\nhave exact comparison across all categories. For example,\\nour taxonomy does not cover “toxic” and Perspective API\\ndoes not explicitly detect “self-harm” or “sexual content”.\\nSee the details on how we match two taxonomies and pre-\\nprocess each test dataset in Appendix. A.\\nIt is not surprising that our model performs the best on the\\ntest dataset labeled with the same taxonomy and the Perspec-\\ntive API does a better job on Jigsaw data. It further proves\\nthe point on how important it is to align the taxonomy be-\\ntween training data and use cases in evaluation. Our model\\noutperforms the Perspective API baseline on both TweetEval\\nand Stormfront test sets for detecting hateful content, despite\\nthe fact that neither are in the training set.\\n4.3 Active Learning Experiments\\nTo assess the importance of active learning, we evaluate the\\nperformance of our active learning strategy, as described in\\n5https://www.perspectiveapi.com/Category Random\\nSamplingActive\\nLearn', 'ingMultiplier\\nS 1.49% 25.53% 17.1 \\x02\\nH 0.17% 3.09% 18.2 \\x02\\nV 0.48% 9.92% 20.7 \\x02\\nHR 0.55% 6.41% 11.7 \\x02\\nSH 0.09% 1.85% 20.6 \\x02\\nS3 0.24% 2.42% 10.1 \\x02\\nH2 0.03% 0.67% 22.3 \\x02\\nV2 0.25% 4.27% 17.1 \\x02\\nSafe 96.57% 59.54% -\\nTable 4: Label distributions for samples selected by random\\nsampling and active learning sampling. Note that one sample\\ncan be assigned with multiple labels so the percentages sum\\nup to more than 100%.\\n§3.1, compared to random sampling.\\nIterative training. We run the following training proce-\\ndure twice, using our active learning strategy and random\\nsampling, respectively.\\n1. Start with an initial training dataset D0ofk0= 6000\\nlabeled examples from public data and a validation set V\\nof about 5500 samples from the production trafﬁc.\\n2. fori 0toN\\x001do (N= 3):\\n(a) Train a new model MionDi;\\n(b) Evaluate MionV;\\n(c) Score 5\\x02105production samples with Mifrom our\\nproduction trafﬁc;\\n(d) Choose about 2000 samples from the above data pool\\nvia the selection strategy in test and add samples to the\\ntraining set to construct Di+1after labeling.\\nResults. Table 4 demonstrates the label distributions ob-\\ntained by the two strategies and our active learning strategy\\ncan capture undesired content 10+ times more effectively\\nthan random sampling on all categories. Overall about 40%\\nof samples selected by active learning can trigger at least one\\nundesired label, while in comparison only 3.4% of random\\nsamples are assigned with any undesired label.\\nAs shown in Fig. 2, using the active learning strategy to\\ndecide which new data samples leads to a greater improve-\\nment across all categories than random sampling. We ob-\\nserve signiﬁcant performance improvement on all categories\\nwith active learning after 3 iterations.\\n4.4 Domain Adversarial Training Experiments\\nWe want to understand the effectiveness of Wasserstein Dis-\\ntance Guided Domain Adversarial Training (WDAT) under\\nthree scenarios: (1) At the beginning of the project, we only\\nhave labeled public data and unlabeled production data. (2)\\nIn the middle of the project, we also curate synthetic exam-\\nples to improve model weaknesses. (3) At the later stage,\\nwe get a sufﬁcient amount of labeled production examples.\\nAll three circumstances are important because we want to Figure 2: Performance of active learning sampling versus\\nrandom sampling on the same validation set at each model\\niteration, measured by AUPRC.\\nmake good use of unlabeled production data to train the best\\nmodel throughout the project, and a strong model on pro-\\nduction trafﬁc boosts the effectiveness of active learning at\\nevery iteration. We use the following setup to compare the\\nperformance on our production trafﬁc.\\nDatasets. We create three training datasets PUB, SYN,\\nand MIX to study (1), (2), and (3), respectively. PUB con-\\nsists of around 90k public examples including both samples\\nfrom academic datasets and Web data (Common Crawl) la-\\nbeled by our annotators. SYN adds additional 69k curated\\nsynthetic examples. MIX contains all examples in SYN wit', 'h\\nadditional 60k production samples with labels.\\nModels. The baseline models are trained with basic super-\\nvised learning. The DAT models are trained with two hidden\\nlayers of 300 dimensions using additional 100k unlabeled\\nproduction data points. All models are trained with up to 2\\nepochs, and the training is repeated 3 times with different\\nrandom seeds.\\nResults. We compare the average AUPRC on the produc-\\ntion validation set V. As demonstrated in Table 5, the im-\\nprovement from WDAT is signiﬁcant when we only haveCategory PUB SYN MIX\\nBaseline DAT Baseline DAT Baseline DAT\\nS .698 .730 .726 .745 .943 .939\\nH .417 .491 .551 .476 .843 .818\\nV .490 .529 .532 .531 .640 .633\\nHR .258 .369 .326 .356 .453 .482\\nSH .063 .281 .086 .296 .621 .632\\nS3 .592 .759 .779 .777 .911 .936\\nH2 .393 .643 .570 .577 .851 .854\\nV2 .165 .453 .093 .507 .443 .533\\nTable 5: The average AUPRC on a production validation\\nset. PUB denotes models trained on labeled public datasets,\\nSYN adds additional synthetic examples, and MIX adds ad-\\nditional labeled production examples. We mark the best re-\\nsult within each conﬁguration in bold .\\naccess to public datasets (PUB), and the marginal gain re-\\nduces gradually as we add more training examples, espe-\\ncially in-distribution production samples. For instance, DAT\\nimproved SHAUPRC from 0.063 to 0.281 on PUB and from\\n0.086 to 0.296 on SYN, whereas the improvement is only\\nfrom 0.621 to 0.632 on MIX. WDAT still helps weak cate-\\ngories ( SHandV2) on SYN and MIX, but it may slightly\\nhurt the performance for categories with a sufﬁcient amount\\nof in-distribution data such as HandV. We suspect this is\\nbecause the model failed to ﬁnd a representation that works\\nvery well for both the public datasets and our production dis-\\ntribution. Further study on the model architecture and train-\\ning methods is required to improve the performance on all\\ncategories with unlabeled data throughout different stages of\\nthe project.\\n5 Related Work\\nThere is a long track record of work on the deﬁnition\\nand detection of hateful, toxic, offensive and abusive con-\\ntent (Kwok and Wang 2013; Nobata et al. 2016; Waseem\\n2016; de Gibert et al. 2018; Vidgen et al. 2019; Gehman\\net al. 2020; Rosenthal et al. 2020; Lees et al. 2022). Zampieri\\net al. (2019) proposed a three-level hierarchical taxonomy\\nconsidering whether the given language is (i) offensive or\\nnot; (ii) targeted or not; and (iii) targeted at a group, an\\nindividual or other organizations. Usually hateful expres-\\nsions targeting protected identity groups are considered hate\\nspeech (Davidson et al. 2017). Perspective API deﬁnes toxi-\\ncity as ”A rude, disrespectful, or unreasonable comment that\\nis likely to make people leave a discussion”. Some also used\\ntoxicity as a general umbrella term for offensive, abusive,\\nand hateful language (Pavlopoulos et al. 2020). The deﬁni-\\ntions of hatefulness, toxicity, offensiveness and abusiveness\\nhave overlaps but are not exactly the same, creating obsta-\\ncles for sharing datasets between ', 'projects. Furthermore, only\\na limited amount of work considered detailed subcategoriza-\\ntions (Mollas et al. 2020; Borkan et al. 2019) to capture a\\nspectrum of severity, making it harder to control labeling\\nquality. Finally, there exist various types of potentially un-\\ndesired text in the wild, such as sexual content involving mi-\\nnors, extreme graphic violence, or support for self-harm or\\nsuicides, besides offensive and abusive language, and we ob- served a gap between current research work and the entirety\\nof content types that should be moderated and detected. Our\\nwork aims to ﬁll in the gap.\\nDespite the common belief that training data quality is\\ncritical for model performance, there is still lack of com-\\nmunity standards for labeling standards, annotator training,\\nquality metrics, etc. (Vidgen and Derczynski 2020; Yin and\\nZubiaga 2021; Lees et al. 2022; PAI 2021). Vidgen and Der-\\nczynski (2020) studied 60+ datasets for abusive language\\ndetection and found that the primary data source is Twit-\\nter and expert coding is the most common way to anno-\\ntate data, closely followed by crowdsourcing. For large-scale\\ndata collection, crowdsourcing remains the most common\\napproach (Mollas et al. 2020; Zampieri et al. 2019; Davidson\\net al. 2017). However, the weak skill set of non-expert anno-\\ntators can lead to lower data quality (Waseem 2016; Yin and\\nZubiaga 2021). Some recent work turns to large pre-trained\\nlanguage models to generate synthetic data, signiﬁcantly re-\\nducing the cost of time and human labor (Wang et al. 2021a;\\nHartvigsen et al. 2022), but it is unclear whether model out-\\nputs would be diverse enough to adapt to the real-world dis-\\ntribution. Synthetic data can be hand-crafted (R ¨ottger et al.\\n2021), but it is limited by size and thus more suitable for\\nevaluation. It is noteworthy that training data can contain\\nbias due to the subjectivity and biases in the data collec-\\ntion process (Davidson, Bhattacharya, and Weber 2019; Sap\\net al. 2019).\\nActive learning has been successfully applied to a num-\\nber of different domains such as text classiﬁcation (Lewis\\nand Gale 1994; Schohn and Cohn 2000; Siddhant and Lipton\\n2018); machine translation (Zeng et al. 2019); image classi-\\nﬁcation (Luo et al. 2005; Hoi et al. 2006; Gal, Islam, and\\nGhahramani 2017); object detection (Schmidt et al. 2020)\\nand information retrieval (Shen and Zhai 2005). There are\\nseveral families of active learning sampling strategies that\\nare often used in practice. Uncertainty sampling selects data\\npoints about which the model is most uncertain. The uncer-\\ntainty of the model can be quantiﬁed by predicted proba-\\nbilities (Lewis and Gale 1994; Lewis and Catlett 1994; Cu-\\nlotta and McCallum 2005; Scheffer, Decomain, and Wro-\\nbel 2001), disagreement among an ensemble of models (Se-\\nung, Opper, and Sompolinsky 1992; Dagan and Engelson\\n1995; McCallum and Nigam 1998), or by using dropout and\\nBayesian approaches (Gal, Islam, and Ghahramani 2017;\\nSiddhant and Lipton 2018). Diversit', 'y sampling chooses\\nsamples in a way that ensures sufﬁcient diversity within\\nthe selection. This is commonly achieved by clustering un-\\nlabeled data and sampling from different clusters (Nguyen\\nand Smeulders 2004; Xu, Akella, and Zhang 2007), or by\\nselecting samples which are ”representative” of the sample\\ndistribution (i.e., which are similar to many other samples)\\n(McCallum and Nigam 1998; Settles and Craven 2008). Un-\\ncertainty and diversity sampling are sometimes combined in\\na single complex active learning strategy.\\nRed-teaming is a common approach for model improve-\\nment by discovering and patching the weakness itera-\\ntively (Dinan et al. 2019; Vidgen et al. 2020; Kiela et al.\\n2021; Ziegler et al. 2022; Perez et al. 2022; Ribeiro et al.\\n2020), where humans are encouraged to look for examplesthat could fail the model. Dynabench (Kiela et al. 2021)\\nis built as a platform for easy adversarial data collection.\\nMishkin et al. (2022) describes in detail an operational pro-\\ncess for doing red-teaming using external experts. Ziegler\\net al. (2022) designed a tool to efﬁciently assist human ad-\\nversaries to identify failures in a classiﬁer. Models trained\\nwith red-teaming data are found to be more robust to ad-\\nversarial attack (Dinan et al. 2019; Ziegler et al. 2022) and\\nhuman-in-the-loop dynamic data collection can efﬁciently\\nimprove model performance (Kiela et al. 2021; Vidgen et al.\\n2020).\\nDomain adaptation aims at generalizing knowledge\\nlearned in the source domain towards a related target do-\\nmain (Ben-David et al. 2006; Weiss, Khoshgoftaar, and\\nWang 2016; Ben-David et al. 2009), the technique is most\\nuseful when there is insufﬁcient labeled data in the target\\ndomain but sufﬁcient labeled data in the source domain. Dif-\\nferent methods have been proposed to transfer the knowl-\\nedge across domains (Ramponi and Plank 2020; Blitzer,\\nMcDonald, and Pereira 2006; Mansour, Mohri, and Ros-\\ntamizadeh 2008). Inspired by generative adversarial nets\\n(GANs) (Goodfellow et al. 2014) which train a discrimina-\\ntor to make the representations of source and target indistin-\\nguishable, Domain Adversarial Training (DAT) methods are\\nproposed to reduce the domain discrepancy through a do-\\nmain discriminator (Arjovsky, Chintala, and Bottou 2017;\\nGanin et al. 2016; Tzeng et al. 2017; Ganin and Lempitsky\\n2015). To learn domain-invariant feature representations,\\nDAT employs a gradient reversal layer to maximize the min-\\nimal loss of the domain discriminator. However, DAT suffers\\nfrom a gradient vanishing problem when the domain dis-\\ncriminator can tell apart the two domains easily, and Wasser-\\nstein distance based methods are proposed to enable a more\\nstable training (Shen et al. 2018; Arjovsky, Chintala, and\\nBottou 2017; Shah et al. 2018).\\n6 Future Work and Limitations\\nBias and Fairness. Similar to other existing NLP mod-\\nels, our models also suffer from bias towards certain de-\\nmographic attributes (Kusner et al. 2017; Garg et al. 2019;\\nDwork et al. 2012). For instance,', ' the model may give higher\\nhate predictions if the input contains gay and higher\\nsexual predictions if the input contains her. This is be-\\ncause we use data from the Internet, and social bias may\\npresent explicitly or implicitly in the training datasets. We\\ntried mitigation methods such as creating a balanced syn-\\nthetic dataset with templates but could not fully eliminate\\nthe issue. In the future, we will continue following related\\nresearch and improve the fairness of our models.\\nData Augmentation. We plan to investigate more data\\naugmentation methods to boost the training dataset. Al-\\nthough our current training dataset naturally includes mis-\\nspelled words and incorrect grammar as some of it is user-\\ngenerated content, it is valuable to experiment with data\\naugmentation to improve lexicon robustness (Wei and Zou\\n2019; Kobayashi 2018; Zhang et al. 2021) and the generaliz-\\nability of the model (Guo, Mao, and Zhang 2019; Shen et al.\\n2020; Gao, Yao, and Chen 2021), especially when working with the changing distribution of real-world data.\\nBetter Multilingual Support. Only about 5% of the sam-\\nples are non-English in our training set. As the vast majority\\nof our production trafﬁc is in English, we have not yet rig-\\norously evaluated or optimized performance on non-English\\ntext. Multilingual toxic content classiﬁcation (Aluru et al.\\n2020; Wang and Banko 2021; Lees et al. 2022) would re-\\nquire more non-English training data and may need addi-\\ntional changes on tokenization or model architecture.\\nRed-teaming at scale. Red-teaming is an effective way to\\nﬁnd unknown failure cases for the model. Currently we do\\ninternal red-teaming with each new model version, which is\\nnot a scalable approach. In the future, we plan to set up a\\npipeline for model red-teaming similar to the one we have\\nfor labeling production trafﬁc. We plan to use a specialized\\ninterface inspired by Kiela et al. (2021); Ziegler et al. (2022)\\nto improve the efﬁciency of the red-teamers.\\nMore Active Learning Experiments. Our current active\\nlearning strategy to select high-value data for labeling is\\nquite simple. For example, we did not explore diversity sam-\\npling due to computational restriction. Onward we plan to\\nrun more rigorous experiments comparing the performance\\nof different active learning strategies, as well as more so-\\nphisticated strategies, incorporating both uncertainty and di-\\nversity sampling.\\n7 Broader Impacts\\nContent moderation classiﬁers have many uses. When paired\\nwith fair and robust enforcement practices, they have the\\npotential to reduce certain instances of misuse6by ensur-\\ning that policies are operationalized on both inputs and out-\\nputs of language models. Classiﬁers also enable ﬁltration of\\ndatasets at scale, which may be used to train language mod-\\nels with desired properties (Welbl et al. 2021) and allow for\\nbetter evaluation of language models (Gehman et al. 2020).\\nLonger-term, content moderation classiﬁers can be used as a\\nway to ensure high-stakes reliabili', 'ty in very-capable AI sys-\\ntems (Ziegler et al. 2022)—a critical necessity for enabling\\nthe deployment of those systems in certain domains.\\nWhile this underscores the importance of the undesired\\ncontent classiﬁers, all classiﬁers rest on certain assumptions\\nand decisions that may present vulnerabilities or make them\\ninappropriate for certain use cases or types of text. Addition-\\nally, these tools can suffer from problematic biases, such as\\ndisproportionate false positives when discussing groups that\\nare frequently the target of hate. (Garg et al. 2019)\\nThe following sections discuss the normative and subjec-\\ntive questions on which these classiﬁers rest and explore the\\nchallenges they present.\\n7.1 Challenges of Taxonomy Design\\nWe take care to design our taxonomy to reﬂect generaliz-\\nable viewpoints. However, much of our data is drawn from\\na US-centric context and the taxonomy was designed to best\\n6misuse may be deﬁned as uses of the model that the moderat-\\ning body does not want to allow, e.g. generation of hateful contentﬁt this data. Additionally, while we have designed our tax-\\nonomy to be as comprehensive as possible, it would still\\nbe useful for future researchers to add and update the cat-\\negories based on their own use cases and deployment con-\\ntexts. Given the sensitive nature of various tasks, we also\\nencourage the use of this taxonomy in concert with other\\nmitigation strategies, as there is no silver bullet for content\\nmoderation.\\nWe hope that this work will encourage further discussion\\nand debate around the principles and values that underpin\\ncontent moderation.\\n7.2 Annotator Viewpoints and Disagreement\\nIt is commonly agreed that the annotation of toxic language\\nis subjective and that annotators’ interpretations may be in-\\nﬂuenced by their personal and cultural backgrounds, includ-\\ning lived experiences, values and demographic factors. For\\nexample, Waseem (2016) found that feminist and anti-racist\\nactivists systematically disagree with crowd workers on their\\nhate speech annotations. In their study, agreement between\\nthe authors, amateurs and expert annotators is low (14 %),\\nmost often because in many instances where the authors had\\nidentiﬁed hate speech, annotators do not.\\nBy necessity, incorporating diverse viewpoints invites dis-\\nagreement on annotation labels. Much of the computer sci-\\nence literature focuses on eliminating inter-rater disagree-\\nments, most often via deliberation or majority vote. How-\\never, in the case of data from or about marginalized popu-\\nlations, disagreement may be a meaningful signal: An ad-\\nverse effect of majority vote in such cases is limiting rep-\\nresentation of minority perspectives in data Prabhakaran,\\nMostafazadeh Davani, and Diaz (2021), potentially reinforc-\\ning societal disparities and harms. Moreover, analyzing dis-\\nagreements may lead to a better understanding of the domain\\nof application Patton et al. (2018).\\nIn their study, rather than aggregating, Davani, D ´ıaz,\\nand Prabhakaran (2021) preserv', 'e annotator disagreements,\\nwhich they note could reﬂect useful and nuanced informa-\\ntion about the uncertainty of a sample’s membership to a\\nclass. Indeed, they demonstrate that their approach yields\\nthe same or better performance than similar approaches with\\naggregated labels, while retaining the ability to estimate un-\\ncertainty in predictions that correlate with real-life annotator\\ndisagreements.\\nMoreover, resolving disagreement via majority vote may\\nbe at odds with preserving minority opinions in subjective\\ntasks. Ovesdotter Alm (2011) argues that achieving a single\\nreal ”ground truth” label is impossible and is not essential\\nin subjective tasks, and calls for ﬁnding ways to model sub-\\njective interpretations of annotators, rather than seeking to\\nreduce the variability in annotations.\\n7.3 Annotator Selection and Welfare\\nWe are committed to ensuring that our labeling tasks are\\nmanaged in a considerate and ethical manner, and we strive\\nto follow current best practices for sourcing data labeling\\nservices (PAI 2021). Via our data vendors, all of our annota-\\ntors are selected for their skill and willingness to participate\\nin these difﬁcult tasks. Before they opt in, all annotators are vetted by counselors and made aware of the risks and poten-\\ntial harms of working with sensitive data. Our data vendors\\nprovide them with access to mental health and wellness re-\\nsources and annotators have the right to opt out at any point.\\n7.4 Data Privacy and Security\\nTrustworthy handling of production data necessitates trans-\\nparency with users and effective security measures. We ob-\\ntain consent from all customers whose data is used to train\\nour moderation models. Customers who wish to opt their\\ndata out of training may do so. No production data is in-\\ncluded in the dataset we are releasing. Our data labeling and\\nactive learning pipelines feature security controls that are de-\\nsigned and tested to protect the conﬁdentiality and integrity\\nof production data. The model we deploy can not be used to\\ngenerate text, only to compute safety scores, so we consider\\nthe risk of training data leakage to be extremely low.\\n7.5 Summary of Broader Impacts Discussion\\nContent moderation classiﬁers are one key tool that em-\\npowers developers of language models at every stage of the\\nmodel development and deployment process- from working\\nwith large-scale datasets, to testing out models, to deploying\\nthe models to many users. However, as we have observed\\nabove, there are a range of normative and subjective deci-\\nsions made throughout the development process of build-\\ning these classiﬁers from designing taxonomies to labeling\\ndata. Given the nature of these tools, these decisions are\\nsometimes distilled down bluntly and do not enable captur-\\ning the nuances that the moderation decision may warrant.\\nThis loss of nuance may disproportionately impact mem-\\nbers of socially marginalized populations by muting their\\nopinions via unweighted majority annotations. This impact\\nis doubly grievous', ' if moderation decisions about members\\nof marginalized populations are made about them by a sys-\\ntem that excludes their input. This highlights some inherent\\nlimitations of classiﬁers, using automated tools for content\\nmoderation, and point to the importance of their robust test-\\ning to ensure suitability for each speciﬁc use that they may\\nbe deployed in.\\n8 Conclusion\\nBuilding high-quality undesired content detection systems\\nin the real world is a challenge that requires the incorpora-\\ntion of multiple methods. A good content taxonomy is the\\nfoundation for problem scoping and data collection. A re-\\nliable data pipeline is needed to guarantee high data qual-\\nity and to handle distribution shift. We show that in cases\\nwhere certain target content occurs rarely, an active learning\\nsampling strategy leads to much better model performance.\\nAdditionally, we argue that good operational aspects of the\\nlabeling pipeline are essential for ensuring high data quality.\\nAnd we show that model performance can further be im-\\nproved through the use of curated synthetic data and semi-\\nsupervised learning.\\nAs large generative language models become more and\\nmore prevalent, it becomes increasingly important to de-\\nvelop ways of controlling and guiding their outputs. Thegoal of this work has been to demonstrate one way of imple-\\nmenting such control by way of building content detection\\nmodels. We are looking forward to further reﬁnement of our\\napproach in the future, as well as progress in other methods\\nof controlling and aligning generative model outputs.\\n9 Acknowledgments\\nThis work would not have been possible without the con-\\ntributions of data workers. We greatly appreciate their work\\nhandling sensitive content and helping us build better auto-\\nmated systems to make content moderation work less de-\\nmanding of human labor.\\nWe also thank Miles Brundage, Raf Jakubanis, Gretchen\\nKrueger, Derek Chen, Summer Yue, Karl Cobbe, Pranav\\nShyam, Jason Kwon and Matt Knight for feedback on this\\nwork.\\nReferences\\nAluru, S. S.; Mathew, B.; Saha, P.; and Mukherjee, A. 2020.\\nDeep learning models for multilingual hate speech detec-\\ntion. arXiv preprint arXiv:2004.06465 .\\nAlzantot, M.; Sharma, Y .; Elgohary, A.; Ho, B.-J.; Srivas-\\ntava, M.; and Chang, K.-W. 2018. Generating Natural Lan-\\nguage Adversarial Examples. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Language\\nProcessing , 2890–2896. Brussels, Belgium: Association for\\nComputational Linguistics.\\nAnaby-Tavor, A.; Carmeli, B.; Goldbraich, E.; Kantor, A.;\\nKour, G.; Shlomov, S.; Tepper, N.; and Zwerdling, N. 2020.\\nDo not have enough data? Deep learning to the rescue! In\\nProceedings of the AAAI Conference on Artiﬁcial Intelli-\\ngence , volume 34, 7383–7390.\\nArjovsky, M.; Chintala, S.; and Bottou, L. 2017. Wasser-\\nstein Generative Adversarial Networks. In Precup, D.; and\\nTeh, Y . W., eds., Proceedings of the 34th International Con-\\nference on Machine Learning , volume 70 of Proceedings of\\nMachine Learning Resea', 'rch , 214–223. PMLR.\\nBarbieri, F.; Camacho-Collados, J.; Espinosa Anke, L.; and\\nNeves, L. 2020. TweetEval: Uniﬁed Benchmark and Com-\\nparative Evaluation for Tweet Classiﬁcation. In Findings\\nof the Association for Computational Linguistics: EMNLP\\n2020 , 1644–1650. Association for Computational Linguis-\\ntics.\\nBarrientos, G. M.; Alaiz-Rodr ´ıguez, R.; Gonz ´alez-Castro,\\nV .; and Parnell, A. C. 2020. Machine learning techniques\\nfor the detection of inappropriate erotic content in text. In-\\nternational Journal of Computational Intelligence Systems ,\\n13(1): 591–603.\\nBen-David, S.; Blitzer, J.; Crammer, K.; Kulesza, A.;\\nPereira, F. C.; and Vaughan, J. W. 2009. A theory of learning\\nfrom different domains. Machine Learning , 79: 151–175.\\nBen-David, S.; Blitzer, J.; Crammer, K.; and Pereira, F. C.\\n2006. Analysis of Representations for Domain Adaptation.\\nInNIPS .\\nBlitzer, J.; McDonald, R. T.; and Pereira, F. C. 2006. Do-\\nmain Adaptation with Structural Correspondence Learning.\\nInEMNLP . Borkan, D.; Dixon, L.; Sorensen, J.; Thain, N.; and Vasser-\\nman, L. 2019. Nuanced Metrics for Measuring Unintended\\nBias with Real Data for Text Classiﬁcation. In Compan-\\nion Proceedings of The 2019 World Wide Web Conference ,\\nWWW ’19, 491–500. New York, NY , USA: Association for\\nComputing Machinery. ISBN 9781450366755.\\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\\nA.; et al. 2020. Language models are few-shot learners. Ad-\\nvances in neural information processing systems , 33: 1877–\\n1901.\\nCohen, A. D.; Roberts, A.; Molina, A.; Butryna, A.; Jin, A.;\\nKulshreshtha, A.; Hutchinson, B.; Zevenbergen, B.; Aguera-\\nArcas, B. H.; ching Chang, C.; Cui, C.; Du, C.; Adiwar-\\ndana, D. D. F.; Chen, D.; Lepikhin, D. D.; Chi, E. H.;\\nHoffman-John, E.; Cheng, H.-T.; Lee, H.; Krivokon, I.; Qin,\\nJ.; Hall, J.; Fenton, J.; Soraker, J.; Meier-Hellstern, K.; Ol-\\nson, K.; Aroyo, L. M.; Bosma, M. P.; Pickett, M. J.; Mene-\\ngali, M. A.; Croak, M.; D ´ıaz, M.; Lamm, M.; Krikun, M.;\\nMorris, M. R.; Shazeer, N.; Le, Q. V .; Bernstein, R.; Ra-\\njakumar, R.; Kurzweil, R.; Thoppilan, R.; Zheng, S.; Bos,\\nT.; Duke, T.; Doshi, T.; Prabhakaran, V .; Rusch, W.; Li, Y .;\\nHuang, Y .; Zhou, Y .; Xu, Y .; and Chen, Z. 2022. LaMDA:\\nLanguage Models for Dialog Applications. In arXiv .\\nCulotta, A.; and McCallum, A. 2005. Reducing labeling ef-\\nfort for structured prediction tasks. In AAAI , volume 5, 746–\\n751.\\nDagan, I.; and Engelson, S. P. 1995. Committee-based sam-\\npling for training probabilistic classiﬁers. In Machine Learn-\\ning Proceedings 1995 , 150–157. Elsevier.\\nDavani, A. M.; D ´ıaz, M.; and Prabhakaran, V . 2021. Dealing\\nwith Disagreements: Looking Beyond the Majority V ote in\\nSubjective Annotations. CoRR , abs/2110.05719.\\nDavidson, T.; Bhattacharya, D.; and Weber, I. 2019. Racial\\nBias in Hate Speech and Abusive Language Detection\\nDatasets. In Proceedings of the Third Workshop on Abu-\\nsive Language Online , 25–35. Florence, Italy: Association', '\\nfor Computational Linguistics.\\nDavidson, T.; Warmsley, D.; Macy, M.; and Weber, I. 2017.\\nAutomated hate speech detection and the problem of offen-\\nsive language. In Proceedings of the international AAAI\\nconference on web and social media , volume 11, 512–515.\\nde Gibert, O.; Perez, N.; Garc ´ıa-Pablos, A.; and Cuadros, M.\\n2018. Hate Speech Dataset from a White Supremacy Forum.\\nInProceedings of the 2nd Workshop on Abusive Language\\nOnline (ALW2) , 11–20. Brussels, Belgium: Association for\\nComputational Linguistics.\\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.\\nBERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding. In Burstein, J.; Doran, C.; and\\nSolorio, T., eds., Proceedings of the 2019 Conference of the\\nNorth American Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies, NAACL-\\nHLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Vol-\\nume 1 (Long and Short Papers) , 4171–4186. Association for\\nComputational Linguistics.Dinan, E.; Humeau, S.; Chintagunta, B.; and Weston, J.\\n2019. Build it break it ﬁx it for dialogue safety: Ro-\\nbustness from adversarial human attack. arXiv preprint\\narXiv:1908.06083 .\\nDwork, C.; Hardt, M.; Pitassi, T.; Reingold, O.; and Zemel,\\nR. 2012. Fairness through Awareness. In Proceedings of\\nthe 3rd Innovations in Theoretical Computer Science Con-\\nference , ITCS ’12, 214–226. New York, NY , USA: Associa-\\ntion for Computing Machinery. ISBN 9781450311151.\\nFeng, S.; Wallace, E.; Grissom II, A.; Iyyer, M.; Rodriguez,\\nP.; and Boyd-Graber, J. 2018. Pathologies of Neural Mod-\\nels Make Interpretations Difﬁcult. In Proceedings of the\\n2018 Conference on Empirical Methods in Natural Lan-\\nguage Processing , 3719–3728. Brussels, Belgium: Associ-\\nation for Computational Linguistics.\\nGal, Y .; Islam, R.; and Ghahramani, Z. 2017. Deep bayesian\\nactive learning with image data. In International Conference\\non Machine Learning , 1183–1192. PMLR.\\nGanin, Y .; and Lempitsky, V . S. 2015. Unsupervised Domain\\nAdaptation by Backpropagation. ArXiv , abs/1409.7495.\\nGanin, Y .; Ustinova, E.; Ajakan, H.; Germain, P.; Larochelle,\\nH.; Laviolette, F.; Marchand, M.; and Lempitsky, V . 2016.\\nDomain-Adversarial Training of Neural Networks. J. Mach.\\nLearn. Res. , 17(1): 2096–2030.\\nGao, T.; Yao, X.; and Chen, D. 2021. SimCSE: Simple Con-\\ntrastive Learning of Sentence Embeddings. In Empirical\\nMethods in Natural Language Processing (EMNLP) .\\nGarg, S.; Perot, V .; Limtiaco, N.; Taly, A.; Chi, E. H.; and\\nBeutel, A. 2019. Counterfactual Fairness in Text Classi-\\nﬁcation through Robustness. In Proceedings of the 2019\\nAAAI/ACM Conference on AI, Ethics, and Society , AIES\\n’19, 219–226. New York, NY , USA: Association for Com-\\nputing Machinery. ISBN 9781450363242.\\nGehman, S.; Gururangan, S.; Sap, M.; Choi, Y .; and\\nSmith, N. A. 2020. Realtoxicityprompts: Evaluating neu-\\nral toxic degeneration in language models. arXiv preprint\\narXiv:2009.11462 .\\nGoodfellow, I. J.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;\\nWarde-', 'Farley, D.; Ozair, S.; Courville, A. C.; and Bengio,\\nY . 2014. Generative Adversarial Nets. In NIPS .\\nGoodfellow, I. J.; Shlens, J.; and Szegedy, C. 2015. Ex-\\nplaining and Harnessing Adversarial Examples. CoRR ,\\nabs/1412.6572.\\nGuo, H.; Mao, Y .; and Zhang, R. 2019. Augmenting data\\nwith mixup for sentence classiﬁcation: An empirical study.\\narXiv preprint arXiv:1905.08941 .\\nHartvigsen, T.; Gabriel, S.; Palangi, H.; Sap, M.; Ray, D.;\\nand Kamar, E. 2022. Toxigen: A large-scale machine-\\ngenerated dataset for adversarial and implicit hate speech\\ndetection. arXiv preprint arXiv:2203.09509 .\\nHoi, S. C.; Jin, R.; Zhu, J.; and Lyu, M. R. 2006. Batch mode\\nactive learning and its application to medical image classiﬁ-\\ncation. In Proceedings of the 23rd international conference\\non Machine learning , 417–424.\\nJigsaw. ???? Perspective API. https://www.perspectiveapi.\\ncom/. Accessed: 2022-06-15. Jigsaw. 2018. Toxic Comment Classiﬁcation Chal-\\nlenge. https://www.kaggle.com/competitions/jigsaw-toxic-\\ncomment-classiﬁcation-challenge/overview. Accessed:\\n2022-06-15.\\nKiela, D.; Bartolo, M.; Nie, Y .; Kaushik, D.; Geiger, A.; Wu,\\nZ.; Vidgen, B.; Prasad, G.; Singh, A.; Ringshia, P.; Ma, Z.;\\nThrush, T.; Riedel, S.; Waseem, Z.; Stenetorp, P.; Jia, R.;\\nBansal, M.; Potts, C.; and Williams, A. 2021. Dynabench:\\nRethinking Benchmarking in NLP. In Proceedings of the\\n2021 Conference of the North American Chapter of the As-\\nsociation for Computational Linguistics: Human Language\\nTechnologies , 4110–4124. Online: Association for Compu-\\ntational Linguistics.\\nKobayashi, S. 2018. Contextual Augmentation: Data Aug-\\nmentation by Words with Paradigmatic Relations. In Pro-\\nceedings of the 2018 Conference of the North American\\nChapter of the Association for Computational Linguistics:\\nHuman Language Technologies, Volume 2 (Short Papers) ,\\n452–457. New Orleans, Louisiana: Association for Compu-\\ntational Linguistics.\\nKumar, V .; Choudhary, A.; and Cho, E. 2020. Data augmen-\\ntation using pre-trained transformer models. arXiv preprint\\narXiv:2003.02245 .\\nKusner, M. J.; Loftus, J.; Russell, C.; and Silva, R. 2017.\\nCounterfactual Fairness. In Guyon, I.; Luxburg, U. V .; Ben-\\ngio, S.; Wallach, H.; Fergus, R.; Vishwanathan, S.; and Gar-\\nnett, R., eds., Advances in Neural Information Processing\\nSystems 30 , 4066–4076. Curran Associates, Inc.\\nKwok, I.; and Wang, Y . 2013. Locate the Hate: De-\\ntecting Tweets against Blacks. In Proceedings of the\\nTwenty-Seventh AAAI Conference on Artiﬁcial Intelligence ,\\nAAAI’13, 1621–1622. AAAI Press.\\nLees, A.; Tran, V . Q.; Tay, Y .; Sorensen, J.; Gupta, J.; Met-\\nzler, D.; and Vasserman, L. 2022. A new generation of\\nperspective api: Efﬁcient multilingual character-level trans-\\nformers. arXiv preprint arXiv:2202.11176 .\\nLewis, D. D.; and Catlett, J. 1994. Heterogeneous uncer-\\ntainty sampling for supervised learning. In Machine learn-\\ning proceedings 1994 , 148–156. Elsevier.\\nLewis, D. D.; and Gale, W. A. 1994. A sequential algorithm\\nfor training text classiﬁers. In SIG', 'IR’94 , 3–12. Springer.\\nLuo, T.; Kramer, K.; Goldgof, D. B.; Hall, L. O.; Samson,\\nS.; Remsen, A.; Hopkins, T.; and Cohn, D. 2005. Active\\nlearning to recognize multiple types of plankton. Journal of\\nMachine Learning Research , 6(4).\\nMansour, Y .; Mohri, M.; and Rostamizadeh, A. 2008. Do-\\nmain Adaptation with Multiple Sources. In NIPS .\\nMcCallum, A.; and Nigam, K. 1998. Employing EM\\nand Pool-Based Active Learning for Text Classiﬁcation.\\nInProceedings of the Fifteenth International Conference\\non Machine Learning , ICML ’98, 350–358. San Fran-\\ncisco, CA, USA: Morgan Kaufmann Publishers Inc. ISBN\\n1558605568.\\nMishkin, P.; Ahmad, L.; Brundage, M.; Krueger, G.; and\\nSastry, G. 2022. DALL·E 2 Preview - Risks and Limitations.Mollas, I.; Chrysopoulou, Z.; Karlos, S.; and Tsoumakas,\\nG. 2020. ETHOS: an online hate speech detection dataset.\\narXiv preprint arXiv:2006.08328 .\\nNguyen, H. T.; and Smeulders, A. 2004. Active learning\\nusing pre-clustering. In Proceedings of the twenty-ﬁrst in-\\nternational conference on Machine learning .\\nNobata, C.; Tetreault, J.; Thomas, A.; Mehdad, Y .; and\\nChang, Y . 2016. Abusive Language Detection in Online\\nUser Content. In Proceedings of the 25th International Con-\\nference on World Wide Web , WWW ’16, 145–153. Republic\\nand Canton of Geneva, CHE: International World Wide Web\\nConferences Steering Committee. ISBN 9781450341431.\\nOvesdotter Alm, C. 2011. Subjective Natural Language\\nProblems: Motivations, Applications, Characterizations, and\\nImplications. In Proceedings of the 49th Annual Meeting of\\nthe Association for Computational Linguistics: Human Lan-\\nguage Technologies , 107–112. Portland, Oregon, USA: As-\\nsociation for Computational Linguistics.\\nPAI. 2021. https://partnershiponai.org/paper/responsible-\\nsourcing-considerations/.\\nPatton, D.; Blandfort, P.; Frey, W.; Gaskell, M.; and Kara-\\nman, S. 2018. Annotating Twitter Data from Vulnera-\\nble Populations: Evaluating Disagreement Between Domain\\nExperts and Graduate Student Annotators.\\nPavlopoulos, J.; Sorensen, J.; Dixon, L.; Thain, N.; and An-\\ndroutsopoulos, I. 2020. Toxicity detection: Does context re-\\nally matter? arXiv preprint arXiv:2006.00998 .\\nPerez, E.; Huang, S.; Song, F.; Cai, T.; Ring, R.; Aslanides,\\nJ.; Glaese, A.; McAleese, N.; and Irving, G. 2022. Red team-\\ning language models with language models. arXiv preprint\\narXiv:2202.03286 .\\nPrabhakaran, V .; Mostafazadeh Davani, A.; and Diaz, M.\\n2021. On Releasing Annotator-Level Labels and Informa-\\ntion in Datasets. In Proceedings of The Joint 15th Linguis-\\ntic Annotation Workshop (LAW) and 3rd Designing Meaning\\nRepresentations (DMR) Workshop . Punta Cana, Dominican\\nRepublic: Association for Computational Linguistics.\\nRamponi, A.; and Plank, B. 2020. Neural Unsuper-\\nvised Domain Adaptation in NLP—A Survey. ArXiv ,\\nabs/2006.00632.\\nReddit. 2022. Building Better Moderator Tools. Accessed:\\n2022-08-04.\\nRibeiro, M. T.; Wu, T.; Guestrin, C.; and Singh, S. 2020.\\nBeyond Accuracy: Behavioral Testing of NLP Models with\\nCheckList. In P', 'roceedings of the 58th Annual Meeting of\\nthe Association for Computational Linguistics , 4902–4912.\\nOnline: Association for Computational Linguistics.\\nRosenthal, S.; Atanasova, P.; Karadzhov, G.; Zampieri,\\nM.; and Nakov, P. 2020. A Large-Scale Semi-Supervised\\nDataset for Offensive Language Identiﬁcation. arXiv\\npreprint arXiv:2004.14454 .\\nR¨ottger, P.; Vidgen, B.; Nguyen, D.; Waseem, Z.; Margetts,\\nH.; and Pierrehumbert, J. 2021. HateCheck: Functional\\nTests for Hate Speech Detection Models. In Proceedings\\nof the 59th Annual Meeting of the Association for Compu-\\ntational Linguistics and the 11th International Joint Con-\\nference on Natural Language Processing (Volume 1: Long Papers) , 41–58. Online: Association for Computational Lin-\\nguistics.\\nSap, M.; Card, D.; Gabriel, S.; Choi, Y .; and Smith, N. A.\\n2019. The Risk of Racial Bias in Hate Speech Detection. In\\nProceedings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics , 1668–1678. Florence, Italy:\\nAssociation for Computational Linguistics.\\nScheffer, T.; Decomain, C.; and Wrobel, S. 2001. Active\\nhidden markov models for information extraction. In Inter-\\nnational Symposium on Intelligent Data Analysis , 309–318.\\nSpringer.\\nSchick, T.; and Sch ¨utze, H. 2021. Generating datasets\\nwith pretrained language models. arXiv preprint\\narXiv:2104.07540 .\\nSchmidt, S.; Rao, Q.; Tatsch, J.; and Knoll, A. 2020. Ad-\\nvanced active learning strategies for object detection. In\\n2020 IEEE Intelligent Vehicles Symposium (IV) , 871–876.\\nIEEE.\\nSchohn, G.; and Cohn, D. 2000. Less is more: Active learn-\\ning with support vector machines. In ICML .\\nSettles, B.; and Craven, M. 2008. An analysis of active\\nlearning strategies for sequence labeling tasks. In proceed-\\nings of the 2008 conference on empirical methods in natural\\nlanguage processing , 1070–1079.\\nSeung, H. S.; Opper, M.; and Sompolinsky, H. 1992. Query\\nby committee. In Proceedings of the ﬁfth annual workshop\\non Computational learning theory , 287–294.\\nShah, D.; Lei, T.; Moschitti, A.; Romeo, S.; and Nakov, P.\\n2018. Adversarial Domain Adaptation for Duplicate Ques-\\ntion Detection. In Proceedings of the 2018 Conference on\\nEmpirical Methods in Natural Language Processing , 1056–\\n1063. Brussels, Belgium: Association for Computational\\nLinguistics.\\nShen, D.; Zheng, M.; Shen, Y .; Qu, Y .; and Chen, W. 2020.\\nA Simple but Tough-to-Beat Data Augmentation Approach\\nfor Natural Language Understanding and Generation. arXiv\\npreprint arXiv:2009.13818 .\\nShen, J.; Qu, Y .; Zhang, W.; and Yu, Y . 2018. Wasser-\\nstein Distance Guided Representation Learning for Domain\\nAdaptation. In AAAI .\\nShen, X.; and Zhai, C. 2005. Active feedback in ad hoc\\ninformation retrieval. In Proceedings of the 28th annual in-\\nternational ACM SIGIR conference on Research and devel-\\nopment in information retrieval , 59–66.\\nSiddhant, A.; and Lipton, Z. C. 2018. Deep Bayesian Ac-\\ntive Learning for Natural Language Processing: Results of a\\nLarge-Scale Empirical Study. In Proceedings of the 2', '018\\nConference on Empirical Methods in Natural Language\\nProcessing , 2904–2909. Brussels, Belgium: Association for\\nComputational Linguistics.\\nSzegedy, C.; Zaremba, W.; Sutskever, I.; Bruna, J.; Erhan,\\nD.; Goodfellow, I. J.; and Fergus, R. 2013. Intriguing prop-\\nerties of neural networks. CoRR , abs/1312.6199.\\nTzeng, E.; Hoffman, J.; Saenko, K.; and Darrell, T. 2017.\\nAdversarial Discriminative Domain Adaptation. 2017 IEEE\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , 2962–2971.Vidgen, B.; and Derczynski, L. 2020. Directions in abu-\\nsive language training data, a systematic review: Garbage\\nin, garbage out. Plos one , 15(12): e0243300.\\nVidgen, B.; Harris, A.; Nguyen, D.; Tromble, R.; Hale, S.;\\nand Margetts, H. 2019. Challenges and frontiers in abusive\\ncontent detection. In Proceedings of the Third Workshop on\\nAbusive Language Online , 80–93. Florence, Italy: Associa-\\ntion for Computational Linguistics.\\nVidgen, B.; Thrush, T.; Waseem, Z.; and Kiela, D.\\n2020. Learning from the worst: Dynamically generated\\ndatasets to improve online hate detection. arXiv preprint\\narXiv:2012.15761 .\\nWang, C.; and Banko, M. 2021. Practical Transformer-based\\nMultilingual Text Classiﬁcation. In Proceedings of the 2021\\nConference of the North American Chapter of the Associa-\\ntion for Computational Linguistics: Human Language Tech-\\nnologies: Industry Papers , 121–129. Online: Association for\\nComputational Linguistics.\\nWang, S.; Liu, Y .; Xu, Y .; Zhu, C.; and Zeng, M. 2021a.\\nWant To Reduce Labeling Cost? GPT-3 Can Help. In\\nFindings of the Association for Computational Linguistics:\\nEMNLP 2021 , 4195–4205. Punta Cana, Dominican Repub-\\nlic: Association for Computational Linguistics.\\nWang, Z.; Yu, A. W.; Firat, O.; and Cao, Y . 2021b.\\nTowards zero-label language learning. arXiv preprint\\narXiv:2109.09193 .\\nWaseem, Z. 2016. Are You a Racist or Am I Seeing Things?\\nAnnotator Inﬂuence on Hate Speech Detection on Twitter.\\nInProceedings of the First Workshop on NLP and Computa-\\ntional Social Science , 138–142. Austin, Texas: Association\\nfor Computational Linguistics.\\nWei, J.; and Zou, K. 2019. EDA: Easy Data Augmentation\\nTechniques for Boosting Performance on Text Classiﬁcation\\nTasks. In Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the 9th Inter-\\nnational Joint Conference on Natural Language Processing\\n(EMNLP-IJCNLP) , 6383–6389. Hong Kong, China: Asso-\\nciation for Computational Linguistics.\\nWeidinger, L.; Mellor, J.; Rauh, M.; Grifﬁn, C.; Uesato, J.;\\nHuang, P.-S.; Cheng, M.; Glaese, M.; Balle, B.; Kasirzadeh,\\nA.; et al. 2021. Ethical and social risks of harm from lan-\\nguage models. arXiv preprint arXiv:2112.04359 .\\nWeiss, K. R.; Khoshgoftaar, T. M.; and Wang, D. 2016. A\\nsurvey of transfer learning. Journal of Big Data , 3: 1–40.\\nWelbl, J.; Glaese, A.; Uesato, J.; Dathathri, S.; Mellor, J.;\\nHendricks, L. A.; Anderson, K.; Kohli, P.; Coppin, B.; and\\nHuang, P.-S. 2021. Challenges in detoxifying language mod-\\nels.arXiv preprin', 't arXiv:2109.07445 .\\nXu, Z.; Akella, R.; and Zhang, Y . 2007. Incorporating di-\\nversity and density in active learning for relevance feedback.\\nInEuropean Conference on Information Retrieval , 246–257.\\nSpringer.\\nYin, W.; and Zubiaga, A. 2021. Towards generalisable hate\\nspeech detection: a review on obstacles and solutions. PeerJ\\nComputer Science , 7: e598. Yoo, K. M.; Park, D.; Kang, J.; Lee, S.-W.; and Park, W.\\n2021. GPT3Mix: Leveraging large-scale language models\\nfor text augmentation. arXiv preprint arXiv:2104.08826 .\\nYouTube. 2019. The Four Rs of Responsibility, Part 1: Re-\\nmoving Harmful Content. Accessed: 2022-08-04.\\nZampieri, M.; Malmasi, S.; Nakov, P.; Rosenthal, S.; Farra,\\nN.; and Kumar, R. 2019. Predicting the Type and Target of\\nOffensive Posts in Social Media. In Proceedings of the 2019\\nConference of the North American Chapter of the Associa-\\ntion for Computational Linguistics: Human Language Tech-\\nnologies, Volume 1 (Long and Short Papers) , 1415–1420.\\nAssociation for Computational Linguistics.\\nZeng, X.; Garg, S.; Chatterjee, R.; Nallasamy, U.; and\\nPaulik, M. 2019. Empirical evaluation of active learning\\ntechniques for neural MT. In Proceedings of the 2nd Work-\\nshop on Deep Learning Approaches for Low-Resource NLP\\n(DeepLo 2019) , 84–93.\\nZhang, C.; Zhao, J.; Zhang, H.; Chang, K.-W.; and Hsieh,\\nC.-J. 2021. Double Perturbation: On the Robustness of Ro-\\nbustness and Counterfactual Bias Evaluation. In Proceed-\\nings of the 2021 Conference of the North American Chap-\\nter of the Association for Computational Linguistics: Hu-\\nman Language Technologies , 3899–3916. Online: Associa-\\ntion for Computational Linguistics.\\nZhao, J.; Wang, T.; Yatskar, M.; Ordonez, V .; and Chang, K.-\\nW. 2017. Men Also Like Shopping: Reducing Gender Bias\\nAmpliﬁcation using Corpus-level Constraints. In Proceed-\\nings of the 2017 Conference on Empirical Methods in Nat-\\nural Language Processing , 2979–2989. Copenhagen, Den-\\nmark: Association for Computational Linguistics.\\nZiegler, D. M.; Nix, S.; Chan, L.; Bauman, T.; Schmidt-\\nNielsen, P.; Lin, T.; Scherlis, A.; Nabeshima, N.; Weinstein-\\nRaun, B.; de Haas, D.; et al. 2022. Adversarial Training for\\nHigh-Stakes Reliability. arXiv preprint arXiv:2205.01663 .\\nA Experiment Details\\nTable 6 presents how we map model taxonomies into la-\\nbels of different evaluation datasets. Some of the mappings\\nare only approximation. For example, Perspective deﬁnes\\n”threat” as ”Describes an intention to inﬂict pain, injury,\\nor violence against an individual or group.”, not includ-\\ning graphic violence, so not a perfect match for our ”vio-\\nlence” category. Either or our taxonomy has a good match\\nfor ”toxic”, ”severe toxic”, or ”offensive.\\nOur Evaluation Set. We are aware that about 4% of our\\nevaluation samples are in non-English. Perspective API call\\ntakes the language as an input parameter, but multilingual is\\nnot supported for several attributes. We instead use ”en” for\\nall the calls.\\nJigsaw. Jigsaw dataset is pretty large and we include about\\n', 'half of it into our training set to resolve the cold-start prob-\\nlem. Among the rest half, we sampled 5000 examples for\\nevaluation.\\nTweetEval. We take the TweetEval (Barbieri et al. 2020)\\ntest datasets7on ”hate” and ”offensive”. There are in total\\n7https://github.com/cardiffnlp/tweeteval/tree/main/datasets2970 samples in the hate task test set and 860 in the offensive\\none.\\nStormfront. We use the test dataset of de Gibert et al.\\n(2018)8, containing 478 samples.\\nReddit. We downsampled 5000 examples from the ”RS -\\n201501” snapshot of Reddit pushshift datasets9and assigned\\nnoisy binary label to each example on whether it contains\\nsexual content according to the subreddits as listed in Barri-\\nentos et al. (2020).\\n8https://github.com/Vicomtech/hate-speech-dataset\\n9https://ﬁles.pushshift.io/reddit/submissions/ Taxonomy Perspective Ours\\nOurs Sexual max(sexually explicit, profanity, ﬂirtation) sexual\\nHate identity attack hate\\nViolence threat violence\\nHarassment max(toxicity, severe toxicity, insult, threat) harassment\\nSexual/minors - sexual/minors\\nJigsaw Toxic toxicity harassment\\nObscene max(sexually explicit, profanity) sexual\\nThreat threat violence\\nInsult insult max(harassment, hate)\\nIdentity hate identity attack hate\\nTweetEval Hate identity attack hate\\nOffensive max(toxicity, severe toxicity, threat, insult,\\nidentity attack)harassment\\nStormfront Hate identity attack hate\\nReddit Sexual max(sexually explicit, profanity, ﬂirtation) sexual\\nTable 6: How taxonomies of different APIs get mapped into labels of various evaluation datasets.']\n"
     ]
    }
   ],
   "source": [
    "text_chunk_size = 3000\n",
    "text_chunks = [full_text[i:i+text_chunk_size] for i in range(0, len(full_text), text_chunk_size)]\n",
    "assert len(text_chunks)*text_chunk_size >= len(full_text), \"Text chunks should be able to cover the full text\"\n",
    "print(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = split_into_chunks(full_text, text_chunk_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 300\n",
      "1 300\n",
      "2 300\n",
      "3 300\n",
      "4 300\n",
      "5 300\n",
      "6 300\n",
      "7 300\n",
      "8 300\n",
      "9 300\n",
      "10 300\n",
      "11 300\n",
      "12 300\n",
      "13 300\n",
      "14 300\n",
      "15 300\n",
      "16 300\n",
      "17 300\n",
      "18 300\n",
      "19 300\n",
      "20 300\n",
      "21 300\n",
      "22 300\n",
      "23 300\n",
      "24 300\n",
      "25 300\n",
      "26 300\n",
      "27 300\n",
      "28 300\n",
      "29 300\n",
      "30 300\n",
      "31 300\n",
      "32 300\n",
      "33 300\n",
      "34 300\n",
      "35 300\n",
      "36 300\n",
      "37 300\n",
      "38 300\n",
      "39 157\n"
     ]
    }
   ],
   "source": [
    "def word_counter(text):\n",
    "    return len(text.split())\n",
    "\n",
    "for i, text_chunk in enumerate(text_chunks):\n",
    "    print(i, word_counter(text_chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3000\n",
      "1 3000\n",
      "2 3000\n",
      "3 3000\n",
      "4 3000\n",
      "5 3000\n",
      "6 3000\n",
      "7 3000\n",
      "8 3000\n",
      "9 3000\n",
      "10 3000\n",
      "11 3000\n",
      "12 3000\n",
      "13 3000\n",
      "14 3000\n",
      "15 3000\n",
      "16 3000\n",
      "17 3000\n",
      "18 3000\n",
      "19 3000\n",
      "20 3000\n",
      "21 3000\n",
      "22 3000\n",
      "23 3000\n",
      "24 3000\n",
      "25 1550\n"
     ]
    }
   ],
   "source": [
    "for i, text_chunk in enumerate(text_chunks):\n",
    "    print(i, len(text_chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(s, text_chunk_size):\n",
    "    words = s.split()\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "    for i, word in enumerate(words):\n",
    "        if i % text_chunk_size == 0 and i > 0:\n",
    "            chunks.append(chunk.strip())\n",
    "            chunk = \"\"\n",
    "        chunk += word + \" \"\n",
    "    if chunk:\n",
    "        chunks.append(chunk.strip())\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mattgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
